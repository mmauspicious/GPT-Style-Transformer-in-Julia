{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "using PyCall\n",
    "torch = pyimport(\"torch\")\n",
    "println(\"PyTorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "using PyCall\n",
    "np = pyimport(\"numpy\")\n",
    "println(\"NumPy version: \", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle, glorot_uniform\n",
    "using Statistics\n",
    "using StatsBase: sample, Weights\n",
    "using Random\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA runtime 12.6, artifact installation\n",
      "CUDA driver 12.7\n",
      "NVIDIA driver 565.90.0\n",
      "\n",
      "CUDA libraries: \n",
      "- CUBLAS: 12.6.4\n",
      "- CURAND: 10.3.7\n",
      "- CUFFT: 11.3.0\n",
      "- CUSOLVER: 11.7.1\n",
      "- CUSPARSE: 12.5.4\n",
      "- CUPTI: 2024.3.2 (API 24.0.0)\n",
      "- NVML: 12.0.0+565.90\n",
      "\n",
      "Julia packages: \n",
      "- CUDA: 5.6.1\n",
      "- CUDA_Driver_jll: 0.10.4+0\n",
      "- CUDA_Runtime_jll: 0.15.5+0\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.11.0\n",
      "- LLVM: 16.0.6\n",
      "\n",
      "Preferences:\n",
      "- CUDA_Runtime_jll.version: 12.6\n",
      "\n",
      "1 device:\n",
      "  0: NVIDIA GeForce MX150 (sm_61, 3.918 GiB / 4.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "using CUDA  # Leveraged CUDA.jl for GPU-accelerated tensor operations.\n",
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using ProgressMeter  # For training progress visualization\n",
    "using Zygote  # For more advanced gradient computation\n",
    "using NNlib  # For additional neural network functions\n",
    "using BSON  # For model serialization\n",
    "using PyCall   # To call Hugging Face Python APIs # Used PyCall to interface with Hugging Face Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Conda\n",
    "# Conda.add(\"datasets\")\n",
    "# Conda.add(\"transformers\")  # Install via Conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_huggingface (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Hugging Face Pipelines Integration\n",
    "# ---------------------------\n",
    "# This section sets up Hugging Face pipelines via PyCall.\n",
    "function init_huggingface_pipelines()\n",
    "    # Use pyimport_conda to auto-install missing packages\n",
    "    transformers = pyimport_conda(\"transformers\", \"transformers\")\n",
    "    huggingface_hub = pyimport_conda(\"huggingface_hub\", \"huggingface_hub\")\n",
    "    \n",
    "    # Import Python modules\n",
    "    os = pyimport(\"os\")\n",
    "    transformers = pyimport(\"transformers\")\n",
    "    huggingface_hub = pyimport(\"huggingface_hub\")\n",
    "    \n",
    "    # Set the Hugging Face token as an environment variable\n",
    "    HF_TOKEN = \"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\"\n",
    "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    \n",
    "    # Login using the token\n",
    "    huggingface_hub.login(token=HF_TOKEN)\n",
    "    \n",
    "    # Create pipelines for various NLP tasks\n",
    "    text_classifier = transformers.pipeline(\"text-classification\", \n",
    "                                              model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    intent_recognition_model = transformers.pipeline(\"zero-shot-classification\", \n",
    "                                                     model=\"facebook/bart-large-mnli\")\n",
    "    ner_model = transformers.pipeline(\"ner\", \n",
    "                                      model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "    \n",
    "    return (text_classifier, intent_recognition_model, ner_model)\n",
    "end\n",
    "\n",
    "# Test the Hugging Face pipelines with a sample text\n",
    "function test_huggingface()\n",
    "    text_classifier, intent_recognition_model, ner_model = init_huggingface_pipelines()\n",
    "    \n",
    "    sample_text = \"I love Julia and machine learning!\"\n",
    "    \n",
    "    println(\"Text Classification:\")\n",
    "    println(text_classifier(sample_text))\n",
    "    \n",
    "    candidate_labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    println(\"\\nZero-Shot Classification:\")\n",
    "    println(intent_recognition_model(sample_text, candidate_labels))\n",
    "    \n",
    "    println(\"\\nNamed Entity Recognition:\")\n",
    "    println(ner_model(sample_text))\n",
    "end\n",
    "\n",
    "# The functions init_huggingface_pipelines() and test_huggingface() set up and test various Hugging Face pipelines (e.g. text classification, zero-shot classification, and NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_hf_dataset (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Hugging Face Datasets Integration\n",
    "# ---------------------------\n",
    "# Loads a dataset from the Hugging Face Hub.\n",
    "function load_hf_dataset(dataset_name::String, config_name::String)\n",
    "    datasets = pyimport(\"datasets\")\n",
    "    # Download the dataset (here we assume a split named \"train\")\n",
    "    dataset = datasets.load_dataset(dataset_name, config_name)\n",
    "    # Convert the \"train\" split into a Julia array of strings.\n",
    "    # (Assuming the text is stored in a field called \"text\")\n",
    "    texts = [string(x[\"text\"]) for x in dataset[\"train\"]]\n",
    "    return texts\n",
    "end\n",
    "\n",
    "# The new function load_hf_dataset(dataset_name, config_name) uses the Hugging Face datasets library (via PyCall) to load a dataset (here, “wikitext-2-raw-v1”) and returns the training texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token Embeddings: \n",
    "    # Learned embeddings with scaling by `√embed_dim`.\n",
    "\n",
    "# Positional Embeddings:\n",
    "    # Options: Learned (`LearnedPositionalEmbedding`) or Sinusoidal.\n",
    "    # RoPE (Rotary Positional Embeddings): Applied rotation to query/key tensors.\n",
    "    # ALiBi: Added attention bias based on token distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALiBiPositionalEncoding"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Enhanced Token and Positional Embeddings with Weight Tying\n",
    "# ---------------------------\n",
    "struct TokenEmbedding\n",
    "    emb::AbstractArray{Float32,2}\n",
    "    dropout::Dropout\n",
    "    scale_factor::Float32  # Scale embeddings by sqrt(dim)\n",
    "end\n",
    "\n",
    "TokenEmbedding(vocab_size::Int, embed_dim::Int; dropout_prob=0.1, scale=true) = \n",
    "    TokenEmbedding(glorot_uniform(embed_dim, vocab_size), Dropout(dropout_prob), scale ? sqrt(Float32(embed_dim)) : 1.0f0)\n",
    "\n",
    "function (te::TokenEmbedding)(x::AbstractArray{Int})\n",
    "    # Returns embedding with dropout applied and optional scaling\n",
    "    return te.dropout(te.emb[:, x] .* te.scale_factor)\n",
    "end\n",
    "\n",
    "# Learned positional embeddings\n",
    "struct LearnedPositionalEmbedding\n",
    "    emb::AbstractArray{Float32,2}\n",
    "    dropout::Dropout\n",
    "    max_seq_len::Int  # Store max sequence length for potential extrapolation\n",
    "end\n",
    "\n",
    "LearnedPositionalEmbedding(seq_len::Int, embed_dim::Int; dropout_prob=0.1) = \n",
    "    LearnedPositionalEmbedding(glorot_uniform(embed_dim, seq_len), Dropout(dropout_prob), seq_len)\n",
    "\n",
    "function (pe::LearnedPositionalEmbedding)(T::Int)\n",
    "    if T > pe.max_seq_len\n",
    "        orig_emb = pe.emb[:, 1:pe.max_seq_len]\n",
    "        extra_positions = T - pe.max_seq_len\n",
    "        last_pos_diff = orig_emb[:, end] - orig_emb[:, end-1]\n",
    "        extra_emb = hcat([orig_emb[:, end] .+ i .* last_pos_diff for i in 1:extra_positions]...)\n",
    "        extended_emb = hcat(orig_emb, extra_emb)\n",
    "        return pe.dropout(extended_emb)\n",
    "    else\n",
    "        return pe.dropout(pe.emb[:, 1:T])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Sinusoidal positional embeddings (alternative to learned)\n",
    "function sinusoidal_position_embedding(seq_len::Int, embed_dim::Int)\n",
    "    pe = zeros(Float32, embed_dim, seq_len)\n",
    "    for pos in 1:seq_len\n",
    "        for i in 0:2:embed_dim-1\n",
    "            freq = 1.0 / (10000.0^(i/embed_dim))\n",
    "            pe[i+1, pos] = sin(pos * freq)\n",
    "            if i+1 < embed_dim\n",
    "                pe[i+2, pos] = cos(pos * freq)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return pe\n",
    "end\n",
    "\n",
    "struct SinusoidalPositionalEmbedding\n",
    "    emb::AbstractArray{Float32,2}\n",
    "    dropout::Dropout\n",
    "    max_seq_len::Int\n",
    "    embed_dim::Int\n",
    "end\n",
    "\n",
    "function SinusoidalPositionalEmbedding(seq_len::Int, embed_dim::Int; dropout_prob=0.1)\n",
    "    emb = sinusoidal_position_embedding(seq_len, embed_dim)\n",
    "    return SinusoidalPositionalEmbedding(emb, Dropout(dropout_prob), seq_len, embed_dim)\n",
    "end\n",
    "\n",
    "function (pe::SinusoidalPositionalEmbedding)(T::Int)\n",
    "    if T > pe.max_seq_len\n",
    "        extended_emb = sinusoidal_position_embedding(T, pe.embed_dim)\n",
    "        return pe.dropout(extended_emb)\n",
    "    else\n",
    "        return pe.dropout(pe.emb[:, 1:T])\n",
    "    end\n",
    "end\n",
    "\n",
    "# ALiBi positional encoding\n",
    "struct ALiBiPositionalEncoding\n",
    "    slopes::AbstractArray{Float32, 1}\n",
    "    max_seq_len::Int\n",
    "end\n",
    "\n",
    "# In the ALiBiPositionalEncoding constructor:\n",
    "function ALiBiPositionalEncoding(num_heads::Int, max_seq_len::Int)\n",
    "    base = 2^(-(8/num_heads))\n",
    "    slopes = [base^(i-1) for i in 1:num_heads]\n",
    "    # Transfer to GPU if available\n",
    "    return ALiBiPositionalEncoding(CUDA.functional() ? CuArray(slopes) : slopes, max_seq_len)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apply_rotary_pos_emb (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Rotary Position Embeddings (RoPE)\n",
    "# ---------------------------\n",
    "struct RotaryPositionEmbedding\n",
    "    dim::Int\n",
    "    max_seq_len::Int\n",
    "    freqs_cos::AbstractArray{Float32, 2}\n",
    "    freqs_sin::AbstractArray{Float32, 2}\n",
    "    base::Float32\n",
    "end\n",
    "\n",
    "function RotaryPositionEmbedding(dim::Int, max_seq_len::Int; base::Float32=10000.0f0)\n",
    "    theta = base .^ (-(0:2:dim-2) ./ dim)\n",
    "    freqs = repeat(theta, 1, max_seq_len) .* repeat(reshape(1:max_seq_len, 1, :), length(theta), 1)\n",
    "    freqs_cos = cos.(freqs)\n",
    "    freqs_sin = sin.(freqs)\n",
    "    return RotaryPositionEmbedding(dim, max_seq_len, freqs_cos, freqs_sin, base)\n",
    "end\n",
    "\n",
    "function extend_rope_context(rope::RotaryPositionEmbedding, new_seq_len::Int)\n",
    "    if new_seq_len <= rope.max_seq_len\n",
    "        return rope\n",
    "    end\n",
    "    theta = rope.base .^ (-(0:2:rope.dim-2) ./ rope.dim)\n",
    "    freqs = repeat(theta, 1, new_seq_len) .* repeat(reshape(1:new_seq_len, 1, :), length(theta), 1)\n",
    "    freqs_cos = cos.(freqs)\n",
    "    freqs_sin = sin.(freqs)\n",
    "    return RotaryPositionEmbedding(rope.dim, new_seq_len, freqs_cos, freqs_sin, rope.base)\n",
    "end\n",
    "\n",
    "function rotate_half(x::AbstractArray{Float32, 4})\n",
    "    head_dim, seq_len, batch_size, num_heads = size(x)\n",
    "    half_dim = head_dim ÷ 2\n",
    "    x1 = x[1:2:head_dim, :, :, :]\n",
    "    x2 = x[2:2:head_dim, :, :, :]\n",
    "    return cat(-x2, x1, dims=1)\n",
    "end\n",
    "\n",
    "function apply_rotary_pos_emb(q::AbstractArray{Float32, 4}, k::AbstractArray{Float32, 4}, \n",
    "                              freqs_cos::AbstractArray{Float32, 2}, freqs_sin::AbstractArray{Float32, 2}, \n",
    "                              T::Int)\n",
    "    head_dim, seq_len, batch_size, num_heads = size(q)\n",
    "    effective_dim = min(head_dim, size(freqs_cos, 1))\n",
    "    effective_seq_len = min(T, size(freqs_cos, 2))\n",
    "    cos_pos = freqs_cos[1:effective_dim, 1:effective_seq_len]\n",
    "    sin_pos = freqs_sin[1:effective_dim, 1:effective_seq_len]\n",
    "    cos_pos = reshape(cos_pos, effective_dim, effective_seq_len, 1, 1)\n",
    "    sin_pos = reshape(sin_pos, effective_dim, effective_seq_len, 1, 1)\n",
    "    if effective_dim < head_dim\n",
    "        cos_pos = vcat(cos_pos, zeros(Float32, head_dim - effective_dim, effective_seq_len, 1, 1))\n",
    "        sin_pos = vcat(sin_pos, zeros(Float32, head_dim - effective_dim, effective_seq_len, 1, 1))\n",
    "    end\n",
    "    if effective_seq_len < seq_len\n",
    "        cos_pos = cat(cos_pos, zeros(Float32, head_dim, seq_len - effective_seq_len, 1, 1), dims=2)\n",
    "        sin_pos = cat(sin_pos, zeros(Float32, head_dim, seq_len - effective_seq_len, 1, 1), dims=2)\n",
    "    end\n",
    "    q_rot = q .* cos_pos .+ rotate_half(q) .* sin_pos\n",
    "    k_rot = k .* cos_pos .+ rotate_half(k) .* sin_pos\n",
    "    return q_rot, k_rot\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention:\n",
    "    # Flash Attention: Optimized memory usage via block-wise computation.\n",
    "    # Causal Masking: Used `CUDA.tril` for GPU-compatible triangular masks.\n",
    "    # Key-Value (KV) Caching: Enabled autoregressive generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Advanced Multi-Head Self-Attention with Flash Attention and ALiBi\n",
    "# ---------------------------\n",
    "struct MultiHeadAttention\n",
    "    W_q::Dense\n",
    "    W_k::Dense\n",
    "    W_v::Dense\n",
    "    W_o::Dense\n",
    "    num_heads::Int\n",
    "    head_dim::Int\n",
    "    attn_dropout::Dropout\n",
    "    resid_dropout::Dropout\n",
    "    rope::Union{RotaryPositionEmbedding, Nothing}\n",
    "    alibi::Union{ALiBiPositionalEncoding, Nothing}\n",
    "    use_flash_attn::Bool\n",
    "    kv_cache::Dict{Symbol, Any}\n",
    "end\n",
    "\n",
    "function MultiHeadAttention(embed_dim::Int, num_heads::Int; \n",
    "                           attn_dropout_prob=0.1, \n",
    "                           resid_dropout_prob=0.1,\n",
    "                           use_rope::Bool=true,\n",
    "                           use_alibi::Bool=false,\n",
    "                           max_seq_len::Int=1024,\n",
    "                           use_flash_attn::Bool=false)\n",
    "    head_dim = div(embed_dim, num_heads)\n",
    "    @assert head_dim * num_heads == embed_dim \"embed_dim must be divisible by num_heads\"\n",
    "    rope = use_rope ? RotaryPositionEmbedding(head_dim, max_seq_len) : nothing\n",
    "    alibi = use_alibi ? ALiBiPositionalEncoding(num_heads, max_seq_len) : nothing\n",
    "    kv_cache = Dict{Symbol, Any}(:keys => nothing, :values => nothing, :seq_len => 0)\n",
    "    return MultiHeadAttention(\n",
    "        Dense(embed_dim, embed_dim),\n",
    "        Dense(embed_dim, embed_dim),\n",
    "        Dense(embed_dim, embed_dim),\n",
    "        Dense(embed_dim, embed_dim),\n",
    "        num_heads,\n",
    "        head_dim,\n",
    "        Dropout(attn_dropout_prob),\n",
    "        Dropout(resid_dropout_prob),\n",
    "        rope,\n",
    "        alibi,\n",
    "        use_flash_attn,\n",
    "        kv_cache\n",
    "    )\n",
    "end\n",
    "\n",
    "function compute_attention(Q::AbstractArray{Float32, 4}, K::AbstractArray{Float32, 4}, V::AbstractArray{Float32, 4}, \n",
    "                           attn_dropout::Dropout, causal_mask::Bool=true, \n",
    "                           alibi::Union{ALiBiPositionalEncoding, Nothing}=nothing)\n",
    "    head_dim, T, batch_size, num_heads = size(Q)\n",
    "    Q_flat = permutedims(Q, (2, 3, 4, 1))\n",
    "    K_flat = permutedims(K, (2, 3, 4, 1))\n",
    "    V_flat = permutedims(V, (2, 3, 4, 1))\n",
    "    Q_flat = reshape(Q_flat, T, batch_size * num_heads, head_dim)\n",
    "    K_flat = reshape(K_flat, T, batch_size * num_heads, head_dim)\n",
    "    V_flat = reshape(V_flat, T, batch_size * num_heads, head_dim)\n",
    "    Q_flat = permutedims(Q_flat, (3, 1, 2))\n",
    "    K_flat = permutedims(K_flat, (3, 1, 2))\n",
    "    V_flat = permutedims(V_flat, (3, 1, 2))\n",
    "    scores = batched_mul(permutedims(Q_flat, (2, 1, 3)), K_flat) ./ sqrt(Float32(head_dim))\n",
    "    if !isnothing(alibi)\n",
    "        \n",
    "        # Before (CPU arrays):\n",
    "        pos_bias = zeros(Float32, T, T, num_heads)\n",
    "        for h in 1:num_heads\n",
    "            for i in 1:T\n",
    "                for j in 1:T\n",
    "                    pos_bias[i, j, h] = -abs(i - j) * alibi.slopes[h]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # After (GPU arrays):\n",
    "        slopes_gpu = alibi.slopes |> gpu  # Ensure slopes are on GPU\n",
    "        i_j = reshape(1:T, 1, :) .- reshape(1:T, :, 1)  # Compute |i-j|\n",
    "        pos_bias = -abs.(i_j) .* reshape(slopes_gpu, 1, 1, :)\n",
    "        pos_bias = CuArray(pos_bias)  # Move to GPU explicitly\n",
    "\n",
    "        pos_bias = reshape(pos_bias, T, T, num_heads, 1)\n",
    "        pos_bias = repeat(pos_bias, 1, 1, 1, batch_size)\n",
    "        pos_bias = reshape(pos_bias, T, T, batch_size * num_heads)\n",
    "        scores = scores .+ pos_bias\n",
    "    end\n",
    "    if causal_mask\n",
    "        # Before (CPU mask):\n",
    "        mask = tril(ones(Float32, T, T))\n",
    "        # After (GPU mask):\n",
    "        mask = CUDA.tril(CUDA.ones(Float32, T, T))\n",
    "        mask = reshape(mask, T, T, 1)\n",
    "        scores = scores .* mask .+ (1.0f0 .- mask) .* -1.0f7\n",
    "    end\n",
    "    attn_weights = softmax(scores, dims=2)\n",
    "    attn_weights = attn_dropout(attn_weights)\n",
    "    out = batched_mul(attn_weights, permutedims(V_flat, (2, 1, 3)))\n",
    "    out = permutedims(out, (2, 1, 3))\n",
    "    out = reshape(out, head_dim, T, batch_size, num_heads)\n",
    "    return out\n",
    "end\n",
    "\n",
    "function flash_attention(Q::AbstractArray{Float32}, K::AbstractArray{Float32}, V::AbstractArray{Float32}, \n",
    "                         attn_dropout::Dropout, causal_mask::Bool=true, block_size::Int=128,\n",
    "                         alibi::Union{ALiBiPositionalEncoding, Nothing}=nothing)\n",
    "    head_dim, T, batch_size, num_heads = size(Q)\n",
    "    O = zeros(Float32, head_dim, T, batch_size, num_heads)\n",
    "    L = zeros(Float32, 1, T, batch_size, num_heads)\n",
    "    m = fill(-Inf32, 1, T, batch_size, num_heads)\n",
    "    for b_q in 1:block_size:T\n",
    "        e_q = min(b_q + block_size - 1, T)\n",
    "        Q_block = Q[:, b_q:e_q, :, :]\n",
    "        max_k_idx = causal_mask ? e_q : T\n",
    "        for b_k in 1:block_size:max_k_idx\n",
    "            e_k = min(b_k + block_size - 1, max_k_idx)\n",
    "            K_block = K[:, b_k:e_k, :, :]\n",
    "            V_block = V[:, b_k:e_k, :, :]\n",
    "            Q_perm = permutedims(Q_block, (3, 4, 2, 1))\n",
    "            K_perm = permutedims(K_block, (3, 4, 2, 1))\n",
    "            V_perm = permutedims(V_block, (3, 4, 2, 1))\n",
    "            scores = batched_mul(Q_perm, permutedims(K_perm, (1, 2, 4, 3))) ./ sqrt(Float32(head_dim))\n",
    "            if !isnothing(alibi)\n",
    "                for h in 1:num_heads\n",
    "                    for i in b_q:e_q\n",
    "                        for j in b_k:e_k\n",
    "                            if i <= T && j <= max_k_idx\n",
    "                                rel_i = i - b_q + 1\n",
    "                                rel_j = j - b_k + 1\n",
    "                                scores[:, h, rel_i, rel_j] .-= abs(i - j) * alibi.slopes[h]\n",
    "                            end\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            if causal_mask\n",
    "                for i in b_q:e_q\n",
    "                    for j in b_k:e_k\n",
    "                        if j > i\n",
    "                            rel_i = i - b_q + 1\n",
    "                            rel_j = j - b_k + 1\n",
    "                            scores[:, :, rel_i, rel_j] .= -1.0f7\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            block_max = maximum(scores, dims=4)\n",
    "            for i in 1:(e_q-b_q+1)\n",
    "                idx = b_q + i - 1\n",
    "                old_m = m[1, idx, :, :]\n",
    "                new_m = max.(old_m, block_max[:, :, i, 1])\n",
    "                scale_factor = exp.(old_m .- new_m)\n",
    "                O[:, idx, :, :] .*= scale_factor\n",
    "                L[1, idx, :, :] .*= scale_factor\n",
    "                m[1, idx, :, :] = new_m\n",
    "            end\n",
    "            for i in 1:(e_q-b_q+1)\n",
    "                idx = b_q + i - 1\n",
    "                exp_scores = exp.(scores[:, :, i, :] .- reshape(m[1, idx, :, :], batch_size, num_heads, 1))\n",
    "                exp_scores = attn_dropout(exp_scores)\n",
    "                weighted_values = batched_mul(exp_scores, V_perm)\n",
    "                weighted_values = permutedims(weighted_values, (4, 1, 2, 3))\n",
    "                O[:, idx, :, :] .+= weighted_values[:, :, :, 1]\n",
    "                L[1, idx, :, :] .+= sum(exp_scores, dims=3)[:, :, 1]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    O ./= L\n",
    "    return O\n",
    "end\n",
    "\n",
    "function (mha::MultiHeadAttention)(x::AbstractArray{Float32,3}, \n",
    "                                   mask::Union{Nothing, AbstractArray{Float32}}=nothing;\n",
    "                                   use_cache::Bool=false,\n",
    "                                   is_causal::Bool=true)\n",
    "    embed_dim, T, batch_size = size(x)\n",
    "    Q = mha.W_q(x)\n",
    "    K = mha.W_k(x)\n",
    "    V = mha.W_v(x)\n",
    "    function split_heads(t)\n",
    "        return reshape(t, mha.head_dim, mha.num_heads, T, batch_size)\n",
    "    end\n",
    "    Qh = permutedims(split_heads(Q), (1, 3, 4, 2))\n",
    "    Kh = permutedims(split_heads(K), (1, 3, 4, 2))\n",
    "    Vh = permutedims(split_heads(V), (1, 3, 4, 2))\n",
    "    if !isnothing(mha.rope)\n",
    "        Qh, Kh = apply_rotary_pos_emb(Qh, Kh, mha.rope.freqs_cos, mha.rope.freqs_sin, T)\n",
    "    end\n",
    "    if use_cache\n",
    "        if isnothing(mha.kv_cache[:keys])\n",
    "            mha.kv_cache[:keys] = Kh\n",
    "            mha.kv_cache[:values] = Vh\n",
    "            mha.kv_cache[:seq_len] = T\n",
    "        else\n",
    "            prev_len = mha.kv_cache[:seq_len]\n",
    "            mha.kv_cache[:keys] = cat(mha.kv_cache[:keys], Kh, dims=2)\n",
    "            mha.kv_cache[:values] = cat(mha.kv_cache[:values], Vh, dims=2)\n",
    "            mha.kv_cache[:seq_len] += T\n",
    "            Kh = mha.kv_cache[:keys]\n",
    "            Vh = mha.kv_cache[:values]\n",
    "        end\n",
    "    end\n",
    "    if mha.use_flash_attn\n",
    "        out = flash_attention(Qh, Kh, Vh, mha.attn_dropout, is_causal, 128, mha.alibi)\n",
    "    else\n",
    "        out = compute_attention(Qh, Kh, Vh, mha.attn_dropout, is_causal, mha.alibi)\n",
    "    end\n",
    "    out = reshape(permutedims(out, (1, 2, 4, 3)), embed_dim, T, batch_size)\n",
    "    return mha.resid_dropout(mha.W_o(out))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Improved MLP with SwiGLU Activation\n",
    "# ---------------------------\n",
    "struct SwiGLU\n",
    "    W_up::Dense\n",
    "    W_gate::Dense\n",
    "    W_down::Dense\n",
    "    dropout::Dropout\n",
    "end\n",
    "\n",
    "function SwiGLU(dim::Int, hidden_dim::Int; dropout_prob=0.1)\n",
    "    return SwiGLU(\n",
    "        Dense(dim, hidden_dim),\n",
    "        Dense(dim, hidden_dim),\n",
    "        Dense(hidden_dim, dim),\n",
    "        Dropout(dropout_prob)\n",
    "    )\n",
    "end\n",
    "\n",
    "function (swiglu::SwiGLU)(x::AbstractArray{Float32})\n",
    "    up = swiglu.W_up(x)\n",
    "    gate = swiglu.W_gate(x)\n",
    "    swish_gate = gate .* sigmoid.(gate)\n",
    "    hidden = swish_gate .* up\n",
    "    return swiglu.dropout(swiglu.W_down(hidden))\n",
    "end\n",
    "\n",
    "struct GeGLU\n",
    "    W_up::Dense\n",
    "    W_gate::Dense\n",
    "    W_down::Dense\n",
    "    dropout::Dropout\n",
    "end\n",
    "\n",
    "function GeGLU(dim::Int, hidden_dim::Int; dropout_prob=0.1)\n",
    "    return GeGLU(\n",
    "        Dense(dim, hidden_dim),\n",
    "        Dense(dim, hidden_dim),\n",
    "        Dense(hidden_dim, dim),\n",
    "        Dropout(dropout_prob)\n",
    "    )\n",
    "end\n",
    "\n",
    "function (geglu::GeGLU)(x::AbstractArray{Float32})\n",
    "    up = geglu.W_up(x)\n",
    "    gate = geglu.W_gate(x)\n",
    "    gelu_gate = NNlib.gelu.(gate)\n",
    "    hidden = gelu_gate .* up\n",
    "    return geglu.dropout(geglu.W_down(hidden))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Enhanced Transformer Block with Pre/Post Norm Options\n",
    "# ---------------------------\n",
    "struct TransformerBlock\n",
    "    attn::MultiHeadAttention\n",
    "    norm1::LayerNorm\n",
    "    ffn::Union{SwiGLU, GeGLU}\n",
    "    norm2::LayerNorm\n",
    "    pre_norm::Bool\n",
    "    residual_scale::Float32\n",
    "end\n",
    "\n",
    "function TransformerBlock(embed_dim::Int, num_heads::Int; \n",
    "                         ffn_dim_mult=4, \n",
    "                         dropout_prob=0.1,\n",
    "                         pre_norm=true,\n",
    "                         use_rope=true,\n",
    "                         use_alibi=false,\n",
    "                         max_seq_len=1024,\n",
    "                         use_flash_attn=false,\n",
    "                         ffn_type=:swiglu,\n",
    "                         residual_scale=1.0f0)\n",
    "    attn = MultiHeadAttention(embed_dim, num_heads, \n",
    "                             attn_dropout_prob=dropout_prob, \n",
    "                             resid_dropout_prob=dropout_prob,\n",
    "                             use_rope=use_rope,\n",
    "                             use_alibi=use_alibi,\n",
    "                             max_seq_len=max_seq_len,\n",
    "                             use_flash_attn=use_flash_attn)\n",
    "    norm1 = LayerNorm(embed_dim)\n",
    "    if ffn_type == :swiglu\n",
    "        ffn = SwiGLU(embed_dim, ffn_dim_mult * embed_dim, dropout_prob=dropout_prob)\n",
    "    else\n",
    "        ffn = GeGLU(embed_dim, ffn_dim_mult * embed_dim, dropout_prob=dropout_prob)\n",
    "    end\n",
    "    norm2 = LayerNorm(embed_dim)\n",
    "    return TransformerBlock(attn, norm1, ffn, norm2, pre_norm, Float32(residual_scale))\n",
    "end\n",
    "\n",
    "function (block::TransformerBlock)(x::AbstractArray{Float32,3}; \n",
    "                                  mask=nothing, \n",
    "                                  use_cache=false,\n",
    "                                  is_causal=true)\n",
    "    if block.pre_norm\n",
    "        attn_output = block.attn(block.norm1(x), mask; use_cache=use_cache, is_causal=is_causal)\n",
    "        x = x + block.residual_scale .* attn_output\n",
    "        x = x + block.residual_scale .* block.ffn(block.norm2(x))\n",
    "    else\n",
    "        x = block.norm1(x + block.residual_scale .* block.attn(x, mask; use_cache=use_cache, is_causal=is_causal))\n",
    "        x = block.norm2(x + block.residual_scale .* block.ffn(x))\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Enhanced GPT Model with Advanced Features\n",
    "# ---------------------------\n",
    "struct AdvancedGPT\n",
    "    token_emb::TokenEmbedding\n",
    "    pos_emb::Union{LearnedPositionalEmbedding, SinusoidalPositionalEmbedding, Nothing}\n",
    "    blocks::Vector{TransformerBlock}\n",
    "    ln_f::LayerNorm\n",
    "    head           # no type annotation, so head can be any callable layer\n",
    "    emb_dropout::Dropout\n",
    "    config::Dict{Symbol, Any}\n",
    "    weight_tying::Bool\n",
    "end\n",
    "\n",
    "struct TiedDense\n",
    "    tied_matrix::AbstractArray{Float32,2}\n",
    "end\n",
    "\n",
    "function (td::TiedDense)(x)\n",
    "    # Reshape 3D input (embed_dim, seq_len, batch_size) → 2D (embed_dim, seq_len * batch_size)\n",
    "    x_2d = reshape(x, size(x, 1), :)\n",
    "    # Perform matrix multiplication\n",
    "    logits_2d = td.tied_matrix' * x_2d\n",
    "    # Reshape back to 3D (vocab_size, seq_len, batch_size)\n",
    "    return reshape(logits_2d, size(td.tied_matrix, 2), size(x, 2), size(x, 3))\n",
    "end\n",
    "\n",
    "function AdvancedGPT(;\n",
    "    vocab_size::Int,\n",
    "    max_seq_len::Int,\n",
    "    embed_dim::Int,\n",
    "    num_heads::Int,\n",
    "    num_layers::Int,\n",
    "    dropout_prob::Float64=0.1,\n",
    "    pos_emb_type::Symbol=:learned,\n",
    "    pre_norm::Bool=true,\n",
    "    use_rope::Bool=true,\n",
    "    use_alibi::Bool=false,\n",
    "    use_flash_attn::Bool=false,\n",
    "    ffn_dim_mult::Int=4,\n",
    "    ffn_type::Symbol=:swiglu,\n",
    "    weight_tying::Bool=true,\n",
    "    residual_scale::Float64=1.0\n",
    ")\n",
    "    config = Dict{Symbol, Any}(\n",
    "        :vocab_size => vocab_size,\n",
    "        :max_seq_len => max_seq_len,\n",
    "        :embed_dim => embed_dim,\n",
    "        :num_heads => num_heads,\n",
    "        :num_layers => num_layers,\n",
    "        :dropout_prob => dropout_prob,\n",
    "        :pos_emb_type => pos_emb_type,\n",
    "        :pre_norm => pre_norm,\n",
    "        :use_rope => use_rope,\n",
    "        :use_flash_attn => use_flash_attn,\n",
    "        :ffn_dim_mult => ffn_dim_mult\n",
    "    )\n",
    "    token_emb = TokenEmbedding(vocab_size, embed_dim, dropout_prob=0.0)\n",
    "    if pos_emb_type == :learned\n",
    "        pos_emb = LearnedPositionalEmbedding(max_seq_len, embed_dim, dropout_prob=0.0)\n",
    "    else\n",
    "        pos_emb = SinusoidalPositionalEmbedding(max_seq_len, embed_dim, dropout_prob=0.0)\n",
    "    end\n",
    "    emb_dropout = Dropout(dropout_prob)\n",
    "    blocks = [\n",
    "        TransformerBlock(\n",
    "            embed_dim, \n",
    "            num_heads, \n",
    "            ffn_dim_mult=ffn_dim_mult, \n",
    "            dropout_prob=dropout_prob, \n",
    "            pre_norm=pre_norm,\n",
    "            use_rope=use_rope,\n",
    "            use_alibi=use_alibi,\n",
    "            max_seq_len=max_seq_len,\n",
    "            use_flash_attn=use_flash_attn,\n",
    "            ffn_type=ffn_type,\n",
    "            residual_scale=residual_scale\n",
    "        ) for _ in 1:num_layers\n",
    "    ]\n",
    "    \n",
    "    ln_f = LayerNorm(embed_dim)\n",
    "    # In AdvancedGPT constructor:\n",
    "    head = weight_tying ? TiedDense(token_emb.emb) : Dense(embed_dim, vocab_size)\n",
    "    return AdvancedGPT(token_emb, pos_emb, blocks, ln_f, head, emb_dropout, config, weight_tying)\n",
    "end\n",
    "\n",
    "function (model::AdvancedGPT)(x::AbstractArray{Int}; return_all_layers=false)\n",
    "    if ndims(x) == 1\n",
    "        T = length(x)\n",
    "        batch_size = 1\n",
    "        x_reshaped = reshape(x, T, 1)\n",
    "    else\n",
    "        T, batch_size = size(x)\n",
    "        x_reshaped = x\n",
    "    end\n",
    "    tok_emb = model.token_emb(x_reshaped)\n",
    "    pos_emb = model.pos_emb(T)\n",
    "    pos_emb_expanded = repeat(pos_emb, outer=[1, 1, batch_size])\n",
    "    h = model.emb_dropout(tok_emb .+ pos_emb_expanded)\n",
    "    activations = return_all_layers ? [h] : nothing\n",
    "    for block in model.blocks\n",
    "        h = block(h)\n",
    "        if return_all_layers\n",
    "            push!(activations, h)\n",
    "        end\n",
    "    end\n",
    "    h = model.ln_f(h)\n",
    "    # Reshape for TiedDense (embed_dim, T, batch_size) → (embed_dim, T * batch_size)\n",
    "    h_reshaped = reshape(h, size(h, 1), :)\n",
    "    logits = model.head(h_reshaped)\n",
    "    # Reshape back to (vocab_size, T, batch_size)\n",
    "    logits = reshape(logits, size(logits, 1), size(h, 2), size(h, 3))\n",
    "    logits = permutedims(logits, (3, 1, 2))\n",
    "    logits = model.head(h)\n",
    "    logits = permutedims(logits, (3, 1, 2))\n",
    "    if return_all_layers\n",
    "        return logits, activations\n",
    "    else\n",
    "        return logits\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Text Generation Utilities\n",
    "# ---------------------------\n",
    "function softmax_safe(x)\n",
    "    if isa(x, Number)\n",
    "        return [1.0f0]\n",
    "    end\n",
    "    x_vec = vec(collect(Float32, x))\n",
    "    exp_x = exp.(x_vec .- maximum(x_vec))\n",
    "    return exp_x ./ sum(exp_x)\n",
    "end\n",
    "\n",
    "function generate(model::AdvancedGPT, \n",
    "                 prompt::Vector{Int}; \n",
    "                 max_new_tokens::Int=100, \n",
    "                 temperature::Float32=1.0f0,\n",
    "                 top_k::Int=0, \n",
    "                 top_p::Float32=0.9f0)\n",
    "    max_len = model.config[:max_seq_len]\n",
    "    tokens = copy(prompt)\n",
    "    for _ in 1:max_new_tokens\n",
    "        context = tokens[max(1, length(tokens) - max_len + 1):end]\n",
    "        context = reshape(context, :, 1)\n",
    "        logits = model(context)\n",
    "        next_token_logits = Array{Float32}(vec(logits[1, :, end]))\n",
    "        if temperature > 0\n",
    "            next_token_logits ./= temperature\n",
    "        else\n",
    "            _, next_token = findmax(next_token_logits)\n",
    "            push!(tokens, next_token)\n",
    "            continue\n",
    "        end\n",
    "        if top_k > 0\n",
    "            sorted_indices = sortperm(next_token_logits, rev=true)\n",
    "            next_token_logits[sorted_indices[top_k+1:end]] .= -Inf32\n",
    "        end\n",
    "        if top_p < 1.0\n",
    "            sorted_indices = sortperm(next_token_logits, rev=true)\n",
    "            sorted_logits = next_token_logits[sorted_indices]\n",
    "            exp_logits = exp.(sorted_logits .- maximum(sorted_logits))\n",
    "            probs = exp_logits ./ sum(exp_logits)\n",
    "            cumulative_probs = cumsum(probs)\n",
    "            cutoff_idx = findfirst(cumulative_probs .> top_p)\n",
    "            if !isnothing(cutoff_idx) && cutoff_idx > 1\n",
    "                next_token_logits[sorted_indices[cutoff_idx:end]] .= -Inf32\n",
    "            end\n",
    "        end\n",
    "        exp_logits = exp.(next_token_logits .- maximum(next_token_logits))\n",
    "        probs = exp_logits ./ sum(exp_logits)\n",
    "        next_token = sample(1:length(probs), Weights(probs))\n",
    "        push!(tokens, next_token)\n",
    "    end\n",
    "    return tokens\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decode (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Tokenization (Simple Character-level for demonstration)\n",
    "# ---------------------------\n",
    "struct CharTokenizer\n",
    "    vocab::Dict{Char, Int}\n",
    "    idx_to_char::Dict{Int, Char}\n",
    "end\n",
    "\n",
    "function CharTokenizer(text::String)\n",
    "    unique_chars = unique(collect(text))\n",
    "    vocab = Dict(char => i for (i, char) in enumerate(unique_chars))\n",
    "    idx_to_char = Dict(i => char for (i, char) in enumerate(unique_chars))\n",
    "    return CharTokenizer(vocab, idx_to_char)\n",
    "end\n",
    "\n",
    "function encode(tokenizer::CharTokenizer, text::String)\n",
    "    return [tokenizer.vocab[c] for c in text if haskey(tokenizer.vocab, c)]\n",
    "end\n",
    "\n",
    "function decode(tokenizer::CharTokenizer, indices::Vector{Int})\n",
    "    return join([tokenizer.idx_to_char[i] for i in indices if haskey(tokenizer.idx_to_char, i)])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_gpt! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Training Loop\n",
    "# ---------------------------\n",
    "function train_gpt!(model::AdvancedGPT, data::Vector{Int}, \n",
    "                   opt::Flux.Optimise.AbstractOptimiser;\n",
    "                   batch_size::Int=16, \n",
    "                   bptt::Int=64,\n",
    "                   epochs::Int=1,\n",
    "                   lr::Float64=3e-4,\n",
    "                   clip_norm::Float64=1.0)\n",
    "    \n",
    "    data_length = length(data)\n",
    "    \n",
    "    function get_batch()\n",
    "        starts = rand(1:(data_length - bptt), batch_size)\n",
    "        xs = zeros(Int, bptt, batch_size)\n",
    "        ys = zeros(Int, bptt, batch_size)\n",
    "        for (i, start) in enumerate(starts)\n",
    "            end_idx = start + bptt - 1\n",
    "            xs[:, i] = data[start:end_idx]\n",
    "            ys[:, i] = data[(start+1):(end_idx+1)]\n",
    "        end\n",
    "        return xs, ys\n",
    "    end\n",
    "    \n",
    "    steps_per_epoch = div(data_length, batch_size * bptt)\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    params = Flux.params(model)\n",
    "    \n",
    "    for epoch in 1:epochs\n",
    "        epoch_loss = 0.0\n",
    "        for step in 1:steps_per_epoch\n",
    "            x_batch, y_batch = get_batch()\n",
    "            loss, grads = Flux.withgradient(params) do\n",
    "                logits = model(x_batch)\n",
    "                logits_flat = reshape(permutedims(logits, (2, 3, 1)), :, batch_size)\n",
    "                targets_flat = reshape(y_batch, :)\n",
    "                return crossentropy(logits_flat, targets_flat)\n",
    "            end\n",
    "            if clip_norm > 0\n",
    "                Flux.Optimise.clip!(grads, clip_norm)\n",
    "            end\n",
    "            Flux.Optimise.update!(opt, params, grads)\n",
    "            epoch_loss += loss\n",
    "            if step % 10 == 0\n",
    "                println(\"Epoch: $epoch, Step: $step/$steps_per_epoch, Loss: $(loss)\")\n",
    "            end\n",
    "        end\n",
    "        avg_loss = epoch_loss / steps_per_epoch\n",
    "        println(\"Epoch $epoch completed. Average loss: $avg_loss\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available and functional\n"
     ]
    },
    {
     "ename": "ErrorException",
     "evalue": "`llvmcall` requires the compiler",
     "output_type": "error",
     "traceback": [
      "`llvmcall` requires the compiler\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0 [inlined]\n",
      "  [2] _pullback(::Zygote.Context{false}, ::Core.IntrinsicFunction, ::Tuple{String, String}, ::Type{Nothing}, ::Type{Tuple{Bool}}, ::Bool)\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:91\n",
      "  [3] assume\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\LLVM\\b3kFs\\src\\interop\\intrinsics.jl:16 [inlined]\n",
      "  [4] driver_version\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\CUDA\\1kIOw\\lib\\cudadrv\\version.jl:20 [inlined]\n",
      "  [5] isvalid\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\CUDA\\1kIOw\\lib\\cudadrv\\context.jl:71 [inlined]\n",
      "  [6] _pullback(ctx::Zygote.Context{false}, f::typeof(CUDA.isvalid), args::CuContext)\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      "  [7] validate_task_local_state\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\CUDA\\1kIOw\\lib\\cudadrv\\state.jl:61 [inlined]\n",
      "  [8] _pullback(ctx::Zygote.Context{false}, f::typeof(CUDA.validate_task_local_state), args::CUDA.TaskLocalState)\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      "  [9] task_local_state!\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\CUDA\\1kIOw\\lib\\cudadrv\\state.jl:72 [inlined]\n",
      " [10] _pullback(::Zygote.Context{false}, ::typeof(CUDA.task_local_state!))\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      " [11] device\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\CUDA\\1kIOw\\lib\\cudadrv\\state.jl:189 [inlined]\n",
      " [12] CuArray\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\CUDA\\1kIOw\\src\\array.jl:75 [inlined]\n",
      " [13] _pullback(::Zygote.Context{false}, ::Type{CuArray{Float32, 2, CUDA.DeviceMemory}}, ::UndefInitializer, ::Tuple{Int64, Int64})\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      " [14] CuArray\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\CUDA\\1kIOw\\src\\array.jl:126 [inlined]\n",
      " [15] CuArray\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\CUDA\\1kIOw\\src\\array.jl:144 [inlined]\n",
      " [16] _pullback(::Zygote.Context{false}, ::Type{CuArray{Float32}}, ::UndefInitializer, ::Int64, ::Int64)\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      " [17] _apply(::Function, ::Vararg{Any})\n",
      "    @ Core .\\boot.jl:946\n",
      " [18] adjoint\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\lib\\lib.jl:202 [inlined]\n",
      " [19] _pullback\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\ZygoteRules\\CkVIK\\src\\adjoint.jl:67 [inlined]\n",
      " [20] ones\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\CUDA\\1kIOw\\src\\array.jl:774 [inlined]\n",
      " [21] _pullback(::Zygote.Context{false}, ::typeof(CUDA.ones), ::Type{Float32}, ::Int64, ::Int64)\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      " [22] compute_attention\n",
      "    @ d:\\code_folder\\New_folder\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X15sZmlsZQ==.jl:88 [inlined]\n",
      " [23] _pullback(::Zygote.Context{false}, ::typeof(compute_attention), ::Array{Float32, 4}, ::Array{Float32, 4}, ::Array{Float32, 4}, ::Dropout{Float64, Colon, TaskLocalRNG}, ::Bool, ::Nothing)\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      " [24] #_#22\n",
      "    @ d:\\code_folder\\New_folder\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X15sZmlsZQ==.jl:202 [inlined]\n",
      " [25] _pullback(::Zygote.Context{false}, ::var\"##_#22\", ::Bool, ::Bool, ::MultiHeadAttention, ::Array{Float32, 3}, ::Nothing)\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      " [26] MultiHeadAttention\n",
      "    @ d:\\code_folder\\New_folder\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X15sZmlsZQ==.jl:168 [inlined]\n",
      " [27] _pullback(::Zygote.Context{false}, ::typeof(Core.kwcall), ::@NamedTuple{use_cache::Bool, is_causal::Bool}, ::MultiHeadAttention, ::Array{Float32, 3}, ::Nothing)\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      " [28] #_#27\n",
      "    @ d:\\code_folder\\New_folder\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X20sZmlsZQ==.jl:45 [inlined]\n",
      " [29] _pullback(::Zygote.Context{false}, ::var\"##_#27\", ::Nothing, ::Bool, ::Bool, ::TransformerBlock, ::Array{Float32, 3})\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      " [30] TransformerBlock\n",
      "    @ d:\\code_folder\\New_folder\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X20sZmlsZQ==.jl:40 [inlined]\n",
      " [31] _pullback(ctx::Zygote.Context{false}, f::TransformerBlock, args::Array{Float32, 3})\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      " [32] #_#31\n",
      "    @ d:\\code_folder\\New_folder\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X21sZmlsZQ==.jl:102 [inlined]\n",
      " [33] _pullback(::Zygote.Context{false}, ::var\"##_#31\", ::Bool, ::AdvancedGPT, ::Matrix{Int64})\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      " [34] AdvancedGPT\n",
      "    @ d:\\code_folder\\New_folder\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X21sZmlsZQ==.jl:87 [inlined]\n",
      " [35] _pullback(ctx::Zygote.Context{false}, f::AdvancedGPT, args::Matrix{Int64})\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      " [36] #57\n",
      "    @ d:\\code_folder\\New_folder\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X25sZmlsZQ==.jl:54 [inlined]\n",
      " [37] _pullback(ctx::Zygote.Context{false}, f::var\"#57#59\", args::AdvancedGPT)\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface2.jl:0\n",
      " [38] pullback(f::Function, cx::Zygote.Context{false}, args::AdvancedGPT)\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface.jl:90\n",
      " [39] pullback\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface.jl:88 [inlined]\n",
      " [40] withgradient(f::Function, args::AdvancedGPT)\n",
      "    @ Zygote C:\\Users\\khoj\\.julia\\packages\\Zygote\\TWpme\\src\\compiler\\interface.jl:205\n",
      " [41] #withgradient#5\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\Flux\\3711C\\src\\gradient.jl:182 [inlined]\n",
      " [42] withgradient\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\Flux\\3711C\\src\\gradient.jl:169 [inlined]\n",
      " [43] optimized_train_gpt!(model::AdvancedGPT, data::Vector{Int64}, opt::Flux.Optimise.Adam; batch_size::Int64, bptt::Int64, epochs::Int64, lr::Float64, clip_norm::Float64)\n",
      "    @ Main d:\\code_folder\\New_folder\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X25sZmlsZQ==.jl:52\n",
      " [44] macro expansion\n",
      "    @ C:\\Users\\khoj\\.julia\\packages\\GPUArraysCore\\aNaXo\\src\\GPUArraysCore.jl:206 [inlined]\n",
      " [45] top-level scope\n",
      "    @ d:\\code_folder\\New_folder\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X25sZmlsZQ==.jl:116"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Optimized Training Loop\n",
    "# ---------------------------\n",
    "function optimized_train_gpt!(model::AdvancedGPT, data::Vector{Int}, \n",
    "                              opt::Flux.Optimise.AbstractOptimiser;\n",
    "                              batch_size::Int=16, \n",
    "                              bptt::Int=64,\n",
    "                              epochs::Int=1,\n",
    "                              lr::Float64=3e-4,\n",
    "                              clip_norm::Float64=1.0)\n",
    "    \n",
    "    data_length = length(data)\n",
    "    \n",
    "    function get_batch()\n",
    "        # Ensure we don't go out of bounds\n",
    "        max_start = data_length - bptt\n",
    "        if max_start < 1\n",
    "            error(\"Data length is too short for the given bptt value\")\n",
    "        end\n",
    "        \n",
    "        starts = rand(1:max_start, batch_size)\n",
    "        xs = zeros(Int, bptt, batch_size)\n",
    "        ys = zeros(Int, bptt, batch_size)\n",
    "        for (i, start) in enumerate(starts)\n",
    "            end_idx = start + bptt - 1\n",
    "            xs[:, i] = data[start:end_idx]\n",
    "            ys[:, i] = data[(start+1):(end_idx+1)]\n",
    "        end\n",
    "        return xs, ys\n",
    "    end\n",
    "    \n",
    "    # Calculate steps more accurately\n",
    "    steps_per_epoch = max(1, div(data_length - bptt, batch_size))\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    \n",
    "    # Set learning rate if the optimizer supports it\n",
    "    if hasfield(typeof(opt), :eta)\n",
    "        opt.eta = lr\n",
    "    end\n",
    "    \n",
    "    for epoch in 1:epochs\n",
    "        epoch_loss = 0.0\n",
    "        for step in 1:steps_per_epoch\n",
    "            x_batch, y_batch = get_batch()\n",
    "            \n",
    "            # Move data to CPU to avoid GPU kernel issues\n",
    "            x_batch = Array(x_batch)\n",
    "            y_batch = Array(y_batch)\n",
    "            \n",
    "            # Use explicit gradient computation with Flux\n",
    "            # Pass the model directly instead of its parameters\n",
    "            loss, grads = Flux.withgradient(model) do m\n",
    "                # Forward pass through the model\n",
    "                logits = m(x_batch)\n",
    "                # Reshape logits for crossentropy calculation\n",
    "                # The model output shape is (seq_len, batch_size, vocab_size)\n",
    "                logits_flat = reshape(logits, size(logits, 3), :)\n",
    "                targets_flat = reshape(y_batch, :)\n",
    "                return Flux.crossentropy(logits_flat, targets_flat)\n",
    "            end\n",
    "            \n",
    "            # Apply gradient clipping if needed\n",
    "            if clip_norm > 0\n",
    "                Flux.Optimise.clip!(grads, clip_norm)\n",
    "            end\n",
    "            \n",
    "            # Update parameters\n",
    "            Flux.Optimise.update!(opt, Flux.params(model), grads)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            if step % 10 == 0\n",
    "                println(\"Epoch: $epoch, Step: $step/$steps_per_epoch, Loss: $(loss)\")\n",
    "            end\n",
    "        end\n",
    "        avg_loss = epoch_loss / steps_per_epoch\n",
    "        println(\"Epoch $epoch completed. Average loss: $avg_loss\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# Ensure CUDA is functional if available\n",
    "if CUDA.functional()\n",
    "    # Use the recommended approach for scalar operations\n",
    "    println(\"CUDA is available and functional\")\n",
    "else\n",
    "    println(\"CUDA is not available, using CPU\")\n",
    "end\n",
    "\n",
    "# Define and initialize the model\n",
    "vocab_size = 10000  # Example vocabulary size\n",
    "max_seq_len = 128  # Example maximum sequence length\n",
    "embed_dim = 256  # Example embedding dimension\n",
    "num_heads = 8  # Example number of attention heads\n",
    "num_layers = 6  # Example number of transformer layers\n",
    "dropout_prob = 0.1  # Example dropout probability\n",
    "\n",
    "model = AdvancedGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    dropout_prob=dropout_prob\n",
    ")\n",
    "\n",
    "# Move model to CPU to avoid GPU kernel issues\n",
    "model = cpu(model)\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "# Ensure data is long enough for training\n",
    "data = collect(1:10000)  # Example data\n",
    "\n",
    "# Define the optimizer with the specified learning rate\n",
    "opt = Flux.Optimise.Adam(3e-4)\n",
    "\n",
    "# Run the training for 5 epochs\n",
    "CUDA.@allowscalar optimized_train_gpt!(model, data, opt, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
