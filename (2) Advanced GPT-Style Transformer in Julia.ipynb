{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Package cuDNN not found in current path.\n",
      "│ - Run `import Pkg; Pkg.add(\"cuDNN\")` to install the cuDNN package, then restart julia.\n",
      "│ - If cuDNN is not installed, some Flux functionalities will not be available when running on the GPU.\n",
      "└ @ FluxCUDAExt C:\\Users\\khoj\\.julia\\packages\\Flux\\Sgc17\\ext\\FluxCUDAExt\\FluxCUDAExt.jl:10\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle, glorot_uniform\n",
    "using Statistics\n",
    "using StatsBase: sample, Weights\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using CUDA  # For GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Enhanced Token and Positional Embeddings\n",
    "# ---------------------------\n",
    "struct TokenEmbedding\n",
    "    emb::AbstractArray{Float32,2}\n",
    "    dropout::Dropout\n",
    "end\n",
    "\n",
    "TokenEmbedding(vocab_size::Int, embed_dim::Int; dropout_prob=0.1) = \n",
    "    TokenEmbedding(glorot_uniform(embed_dim, vocab_size), Dropout(dropout_prob))\n",
    "\n",
    "function (te::TokenEmbedding)(x::AbstractArray{Int})\n",
    "    # Returns embedding with dropout applied\n",
    "    return te.dropout(te.emb[:, x])\n",
    "end\n",
    "\n",
    "# Learned positional embeddings (like GPT-2)\n",
    "struct LearnedPositionalEmbedding\n",
    "    emb::AbstractArray{Float32,2}\n",
    "    dropout::Dropout\n",
    "end\n",
    "\n",
    "LearnedPositionalEmbedding(seq_len::Int, embed_dim::Int; dropout_prob=0.1) = \n",
    "    LearnedPositionalEmbedding(glorot_uniform(embed_dim, seq_len), Dropout(dropout_prob))\n",
    "\n",
    "function (pe::LearnedPositionalEmbedding)(T::Int)\n",
    "    # Return positional embeddings for positions 1:T with dropout\n",
    "    return pe.dropout(pe.emb[:, 1:T])\n",
    "end\n",
    "\n",
    "# Sinusoidal positional embeddings (alternative to learned) (like the original Transformer)\n",
    "function sinusoidal_position_embedding(seq_len::Int, embed_dim::Int)\n",
    "    # Create PE matrix\n",
    "    pe = zeros(Float32, embed_dim, seq_len)\n",
    "    \n",
    "    # Calculate frequencies\n",
    "    for pos in 1:seq_len\n",
    "        for i in 0:2:embed_dim-1\n",
    "            freq = 1.0 / (10000.0^(i/embed_dim))\n",
    "            pe[i+1, pos] = sin(pos * freq)\n",
    "            if i+1 < embed_dim\n",
    "                pe[i+2, pos] = cos(pos * freq)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return pe\n",
    "end\n",
    "\n",
    "struct SinusoidalPositionalEmbedding\n",
    "    emb::AbstractArray{Float32,2}\n",
    "    dropout::Dropout\n",
    "end\n",
    "\n",
    "function SinusoidalPositionalEmbedding(seq_len::Int, embed_dim::Int; dropout_prob=0.1)\n",
    "    emb = sinusoidal_position_embedding(seq_len, embed_dim)\n",
    "    return SinusoidalPositionalEmbedding(emb, Dropout(dropout_prob))\n",
    "end\n",
    "\n",
    "function (pe::SinusoidalPositionalEmbedding)(T::Int)\n",
    "    return pe.dropout(pe.emb[:, 1:T])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apply_rotary_pos_emb (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Rotary Position Embeddings (RoPE)\n",
    "# ---------------------------\n",
    "\n",
    "# a modern technique used in models like LLaMA that enables better handling of sequences longer than those seen during training\n",
    "\n",
    "struct RotaryPositionEmbedding\n",
    "    dim::Int\n",
    "    max_seq_len::Int\n",
    "    freqs_cos::AbstractArray{Float32, 2}\n",
    "    freqs_sin::AbstractArray{Float32, 2}\n",
    "end\n",
    "\n",
    "function RotaryPositionEmbedding(dim::Int, max_seq_len::Int)\n",
    "    # Generate frequency pairs for rotary embeddings\n",
    "    theta = 10000.0 .^ (-(0:2:dim-2) ./ dim)\n",
    "    freqs = repeat(theta, 1, max_seq_len) .* repeat(reshape(1:max_seq_len, 1, :), length(theta), 1)\n",
    "    freqs_cos = cos.(freqs)\n",
    "    freqs_sin = sin.(freqs)\n",
    "    \n",
    "    return RotaryPositionEmbedding(dim, max_seq_len, freqs_cos, freqs_sin)\n",
    "end\n",
    "\n",
    "function rotate_half(x::AbstractArray{Float32, 4})\n",
    "    # For 4D tensor: (head_dim, seq_len, batch_size, num_heads)\n",
    "    head_dim, seq_len, batch_size, num_heads = size(x)\n",
    "    \n",
    "    # Ensure head_dim is even\n",
    "    half_dim = head_dim ÷ 2\n",
    "    \n",
    "    # Split the first dimension (head_dim) in half\n",
    "    x1 = x[1:2:head_dim, :, :, :]  # First half\n",
    "    x2 = x[2:2:head_dim, :, :, :]  # Second half\n",
    "    \n",
    "    # Stack with negation\n",
    "    return cat(\n",
    "        -x2,  # Negate second half\n",
    "        x1,   # Keep first half as is\n",
    "        dims=1\n",
    "    )\n",
    "end\n",
    "\n",
    "function apply_rotary_pos_emb(q::AbstractArray{Float32, 4}, k::AbstractArray{Float32, 4}, \n",
    "                            freqs_cos::AbstractArray{Float32, 2}, freqs_sin::AbstractArray{Float32, 2}, \n",
    "                            T::Int)\n",
    "    # Apply rotary embeddings to queries and keys\n",
    "    # q, k shape: (head_dim, T, batch_size, num_heads)\n",
    "    head_dim, seq_len, batch_size, num_heads = size(q)\n",
    "    \n",
    "    # Ensure we don't try to access beyond the precomputed frequencies\n",
    "    effective_dim = min(head_dim, size(freqs_cos, 1))\n",
    "    effective_seq_len = min(T, size(freqs_cos, 2))\n",
    "    \n",
    "    # Reshape for broadcasting\n",
    "    cos_pos = freqs_cos[1:effective_dim, 1:effective_seq_len]\n",
    "    sin_pos = freqs_sin[1:effective_dim, 1:effective_seq_len]\n",
    "    \n",
    "    # Explicitly specify dimensions when reshaping\n",
    "    cos_pos = reshape(cos_pos, effective_dim, effective_seq_len, 1, 1)\n",
    "    sin_pos = reshape(sin_pos, effective_dim, effective_seq_len, 1, 1)\n",
    "    \n",
    "    # Pad if necessary to match input dimensions\n",
    "    if effective_dim < head_dim\n",
    "        cos_pos = vcat(cos_pos, zeros(Float32, head_dim - effective_dim, effective_seq_len, 1, 1))\n",
    "        sin_pos = vcat(sin_pos, zeros(Float32, head_dim - effective_dim, effective_seq_len, 1, 1))\n",
    "    end\n",
    "    \n",
    "    if effective_seq_len < seq_len\n",
    "        cos_pos = cat(cos_pos, zeros(Float32, head_dim, seq_len - effective_seq_len, 1, 1), dims=2)\n",
    "        sin_pos = cat(sin_pos, zeros(Float32, head_dim, seq_len - effective_seq_len, 1, 1), dims=2)\n",
    "    end\n",
    "    \n",
    "    # Apply rotary embeddings\n",
    "    q_rot = q .* cos_pos .+ rotate_half(q) .* sin_pos\n",
    "    k_rot = k .* cos_pos .+ rotate_half(k) .* sin_pos\n",
    "    \n",
    "    return q_rot, k_rot\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Advanced Multi-Head Self-Attention with Flash Attention\n",
    "# ---------------------------\n",
    "struct MultiHeadAttention\n",
    "    W_q::Dense\n",
    "    W_k::Dense\n",
    "    W_v::Dense\n",
    "    W_o::Dense\n",
    "    num_heads::Int\n",
    "    head_dim::Int\n",
    "    attn_dropout::Dropout\n",
    "    resid_dropout::Dropout\n",
    "    rope::Union{RotaryPositionEmbedding, Nothing}\n",
    "    use_flash_attn::Bool\n",
    "end\n",
    "\n",
    "function MultiHeadAttention(embed_dim::Int, num_heads::Int; \n",
    "                           attn_dropout_prob=0.1, \n",
    "                           resid_dropout_prob=0.1,\n",
    "                           use_rope::Bool=true,\n",
    "                           max_seq_len::Int=1024,\n",
    "                           use_flash_attn::Bool=false)\n",
    "    head_dim = div(embed_dim, num_heads)\n",
    "    @assert head_dim * num_heads == embed_dim \"embed_dim must be divisible by num_heads\"\n",
    "    \n",
    "    rope = use_rope ? RotaryPositionEmbedding(head_dim, max_seq_len) : nothing\n",
    "    \n",
    "    return MultiHeadAttention(\n",
    "        Dense(embed_dim, embed_dim),  # query\n",
    "        Dense(embed_dim, embed_dim),  # key\n",
    "        Dense(embed_dim, embed_dim),  # value\n",
    "        Dense(embed_dim, embed_dim),  # output\n",
    "        num_heads,\n",
    "        head_dim,\n",
    "        Dropout(attn_dropout_prob),\n",
    "        Dropout(resid_dropout_prob),\n",
    "        rope,\n",
    "        use_flash_attn\n",
    "    )\n",
    "end\n",
    "\n",
    "# Helper function to compute standard attention\n",
    "function compute_attention(Q::AbstractArray{Float32, 4}, K::AbstractArray{Float32, 4}, V::AbstractArray{Float32, 4}, \n",
    "                         attn_dropout::Dropout, causal_mask::Bool=true)\n",
    "    # Q, K, V shapes: (head_dim, T, batch_size, num_heads)\n",
    "    head_dim, T, batch_size, num_heads = size(Q)\n",
    "    \n",
    "    # Reshape to (batch_size * num_heads, T, head_dim)\n",
    "    Q_flat = permutedims(Q, (2, 3, 4, 1))  # (T, batch_size, num_heads, head_dim)\n",
    "    K_flat = permutedims(K, (2, 3, 4, 1))\n",
    "    V_flat = permutedims(V, (2, 3, 4, 1))\n",
    "    \n",
    "    Q_flat = reshape(Q_flat, T, batch_size * num_heads, head_dim)\n",
    "    K_flat = reshape(K_flat, T, batch_size * num_heads, head_dim)\n",
    "    V_flat = reshape(V_flat, T, batch_size * num_heads, head_dim)\n",
    "    \n",
    "    # Transpose Q and K for matrix multiplication\n",
    "    Q_flat = permutedims(Q_flat, (3, 1, 2))  # (head_dim, T, batch_size * num_heads)\n",
    "    K_flat = permutedims(K_flat, (3, 1, 2))\n",
    "    V_flat = permutedims(V_flat, (3, 1, 2))\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = batched_mul(permutedims(Q_flat, (2, 1, 3)), K_flat) ./ sqrt(Float32(head_dim))\n",
    "    \n",
    "    # Apply causal mask if needed\n",
    "    if causal_mask\n",
    "        mask = tril(ones(Float32, T, T))\n",
    "        mask = reshape(mask, T, T, 1)\n",
    "        scores = scores .* mask .+ (1.0f0 .- mask) .* -1.0f7\n",
    "    end\n",
    "    \n",
    "    # Apply softmax and dropout\n",
    "    attn_weights = softmax(scores, dims=2)\n",
    "    attn_weights = attn_dropout(attn_weights)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    out = batched_mul(attn_weights, permutedims(V_flat, (2, 1, 3)))\n",
    "    \n",
    "    # Reshape back to original dimensions (head_dim, T, batch_size, num_heads)\n",
    "    out = permutedims(out, (2, 1, 3))  # (head_dim, T, batch_size * num_heads)\n",
    "    out = reshape(out, head_dim, T, batch_size, num_heads)\n",
    "    \n",
    "    return out\n",
    "end\n",
    "\n",
    "# Simple implementation of flash attention algorithm (approximation for educational purposes)\n",
    "# Flash Attention implementation - an approximation of the algorithm that reduces memory usage and improves\n",
    "function flash_attention(Q::AbstractArray{Float32}, K::AbstractArray{Float32}, V::AbstractArray{Float32}, \n",
    "                         attn_dropout::Dropout, causal_mask::Bool=true, block_size::Int=128)\n",
    "    head_dim, T, batch_size, num_heads = size(Q)\n",
    "    \n",
    "    # Flash attention works by splitting the sequence into chunks/blocks\n",
    "    O = zeros(Float32, head_dim, T, batch_size, num_heads)\n",
    "    L = zeros(Float32, 1, T, batch_size, num_heads)\n",
    "    \n",
    "    for b_q in 1:block_size:T\n",
    "        e_q = min(b_q + block_size - 1, T)\n",
    "        Q_block = Q[:, b_q:e_q, :, :]\n",
    "        \n",
    "        max_k_idx = causal_mask ? e_q : T\n",
    "        \n",
    "        for b_k in 1:block_size:max_k_idx\n",
    "            e_k = min(b_k + block_size - 1, max_k_idx)\n",
    "            K_block = K[:, b_k:e_k, :, :]\n",
    "            V_block = V[:, b_k:e_k, :, :]\n",
    "            \n",
    "            # Compute attention scores for this block pair\n",
    "            Q_perm = permutedims(Q_block, (3, 4, 2, 1))\n",
    "            K_perm = permutedims(K_block, (3, 4, 2, 1))\n",
    "            V_perm = permutedims(V_block, (3, 4, 2, 1))\n",
    "            \n",
    "            # Matrix multiply Q and K\n",
    "            scores = batched_mul(Q_perm, permutedims(K_perm, (1, 2, 4, 3))) ./ sqrt(Float32(head_dim))\n",
    "            \n",
    "            # Apply softmax and attention calculation within the block\n",
    "            block_attn = softmax(scores, dims=4)\n",
    "            block_attn = attn_dropout(block_attn)\n",
    "            block_out = batched_mul(block_attn, V_perm)\n",
    "            \n",
    "            # Update output and normalizers\n",
    "            block_out_perm = permutedims(block_out, (4, 3, 1, 2))\n",
    "            O[:, b_q:e_q, :, :] .+= block_out_perm\n",
    "            L[1, b_q:e_q, :, :] .+= sum(block_attn, dims=4)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Normalize by the sum of attention weights\n",
    "    L = reshape(L, 1, T, batch_size, num_heads)\n",
    "    O ./= L\n",
    "    \n",
    "    return O\n",
    "end\n",
    "\n",
    "function (mha::MultiHeadAttention)(x::AbstractArray{Float32,3}, \n",
    "                                   mask::Union{Nothing, AbstractArray{Float32}}=nothing)\n",
    "    # x has shape (embed_dim, T, batch_size)\n",
    "    embed_dim, T, batch_size = size(x)\n",
    "    \n",
    "    # Compute Q, K, V; shape: (embed_dim, T, batch_size)\n",
    "    Q = mha.W_q(x)\n",
    "    K = mha.W_k(x)\n",
    "    V = mha.W_v(x)\n",
    "    \n",
    "    # Reshape to (head_dim, num_heads, T, batch_size) and then to (head_dim, T, batch_size, num_heads)\n",
    "    function split_heads(t)\n",
    "        return reshape(t, mha.head_dim, mha.num_heads, T, batch_size)\n",
    "    end\n",
    "    Qh = permutedims(split_heads(Q), (1, 3, 4, 2))\n",
    "    Kh = permutedims(split_heads(K), (1, 3, 4, 2))\n",
    "    Vh = permutedims(split_heads(V), (1, 3, 4, 2))\n",
    "    \n",
    "    # Apply rotary positional embeddings if used\n",
    "    if !isnothing(mha.rope)\n",
    "        Qh, Kh = apply_rotary_pos_emb(Qh, Kh, mha.rope.freqs_cos, mha.rope.freqs_sin, T)\n",
    "    end\n",
    "    \n",
    "    # Compute attention - select implementation\n",
    "    if mha.use_flash_attn\n",
    "        out = flash_attention(Qh, Kh, Vh, mha.attn_dropout)\n",
    "    else\n",
    "        out = compute_attention(Qh, Kh, Vh, mha.attn_dropout)\n",
    "    end\n",
    "    \n",
    "    # Reshape back to (embed_dim, T, batch_size)\n",
    "    out = reshape(permutedims(out, (1, 2, 4, 3)), embed_dim, T, batch_size)\n",
    "    \n",
    "    # Final linear projection with dropout\n",
    "    return mha.resid_dropout(mha.W_o(out))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Improved MLP with SwiGLU Activation\n",
    "# ---------------------------\n",
    "\n",
    "# SwiGLU activation function instead of simple ReLU - this activation (Swish-Gated Linear Unit) is used in state-of-the-art models like PaLM and significantly improves performance\n",
    "\n",
    "struct SwiGLU\n",
    "    W_up::Dense\n",
    "    W_gate::Dense\n",
    "    W_down::Dense\n",
    "    dropout::Dropout\n",
    "end\n",
    "\n",
    "function SwiGLU(dim::Int, hidden_dim::Int; dropout_prob=0.1)\n",
    "    return SwiGLU(\n",
    "        Dense(dim, hidden_dim),\n",
    "        Dense(dim, hidden_dim),\n",
    "        Dense(hidden_dim, dim),\n",
    "        Dropout(dropout_prob)\n",
    "    )\n",
    "end\n",
    "\n",
    "function (swiglu::SwiGLU)(x::AbstractArray{Float32})\n",
    "    # SwiGLU activation: SwiGLU(x) = Swish(W_gate*x) ⊗ (W_up*x)\n",
    "    # where Swish(x) = x * sigmoid(x)\n",
    "    up = swiglu.W_up(x)\n",
    "    gate = swiglu.W_gate(x)\n",
    "    swish_gate = gate .* sigmoid.(gate)  # Swish activation\n",
    "    hidden = swish_gate .* up\n",
    "    return swiglu.dropout(swiglu.W_down(hidden))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Enhanced Transformer Block with Pre/Post Norm Options\n",
    "# ---------------------------\n",
    "\n",
    "# Pre-norm (used in GPT) vs Post-norm (original Transformer) layer normalization placement, improving training stability\n",
    "\n",
    "struct TransformerBlock\n",
    "    attn::MultiHeadAttention\n",
    "    norm1::LayerNorm\n",
    "    ffn::SwiGLU\n",
    "    norm2::LayerNorm\n",
    "    pre_norm::Bool\n",
    "end\n",
    "\n",
    "function TransformerBlock(embed_dim::Int, num_heads::Int; \n",
    "                         ffn_dim_mult=4, \n",
    "                         dropout_prob=0.1,\n",
    "                         pre_norm=true,  # Pre-norm (original GPT) or post-norm (original BERT)\n",
    "                         use_rope=true,\n",
    "                         max_seq_len=1024,\n",
    "                         use_flash_attn=false)\n",
    "    attn = MultiHeadAttention(embed_dim, num_heads, \n",
    "                             attn_dropout_prob=dropout_prob, \n",
    "                             resid_dropout_prob=dropout_prob,\n",
    "                             use_rope=use_rope,\n",
    "                             max_seq_len=max_seq_len,\n",
    "                             use_flash_attn=use_flash_attn)\n",
    "    \n",
    "    norm1 = LayerNorm(embed_dim)\n",
    "    ffn = SwiGLU(embed_dim, ffn_dim_mult * embed_dim, dropout_prob=dropout_prob)\n",
    "    norm2 = LayerNorm(embed_dim)\n",
    "    \n",
    "    return TransformerBlock(attn, norm1, ffn, norm2, pre_norm)\n",
    "end\n",
    "\n",
    "function (block::TransformerBlock)(x::AbstractArray{Float32,3}, mask=nothing)\n",
    "    # x: (embed_dim, T, batch_size)\n",
    "    if block.pre_norm\n",
    "        # Pre-normalization architecture (GPT style)\n",
    "        x = x + block.attn(block.norm1(x), mask)\n",
    "        x = x + block.ffn(block.norm2(x))\n",
    "    else\n",
    "        # Post-normalization architecture (original Transformer style)\n",
    "        x = block.norm1(x + block.attn(x, mask))\n",
    "        x = block.norm2(x + block.ffn(x))\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Enhanced GPT Model\n",
    "# ---------------------------\n",
    "struct AdvancedGPT\n",
    "    token_emb::TokenEmbedding\n",
    "    pos_emb::Union{LearnedPositionalEmbedding, SinusoidalPositionalEmbedding}\n",
    "    blocks::Vector{TransformerBlock}\n",
    "    ln_f::LayerNorm\n",
    "    head::Dense\n",
    "    emb_dropout::Dropout\n",
    "    config::Dict{Symbol, Any}\n",
    "end\n",
    "\n",
    "function AdvancedGPT(;\n",
    "    vocab_size::Int,\n",
    "    max_seq_len::Int,\n",
    "    embed_dim::Int,\n",
    "    num_heads::Int,\n",
    "    num_layers::Int,\n",
    "    dropout_prob::Float64=0.1,\n",
    "    pos_emb_type::Symbol=:learned,  # :learned or :sinusoidal\n",
    "    pre_norm::Bool=true,\n",
    "    use_rope::Bool=true,\n",
    "    use_flash_attn::Bool=false,\n",
    "    ffn_dim_mult::Int=4\n",
    ")\n",
    "    config = Dict{Symbol, Any}(\n",
    "        :vocab_size => vocab_size,\n",
    "        :max_seq_len => max_seq_len,\n",
    "        :embed_dim => embed_dim,\n",
    "        :num_heads => num_heads,\n",
    "        :num_layers => num_layers,\n",
    "        :dropout_prob => dropout_prob,\n",
    "        :pos_emb_type => pos_emb_type,\n",
    "        :pre_norm => pre_norm,\n",
    "        :use_rope => use_rope,\n",
    "        :use_flash_attn => use_flash_attn,\n",
    "        :ffn_dim_mult => ffn_dim_mult\n",
    "    )\n",
    "    \n",
    "    token_emb = TokenEmbedding(vocab_size, embed_dim, dropout_prob=0.0)  # No dropout here, we'll apply after sum\n",
    "    \n",
    "    if pos_emb_type == :learned\n",
    "        pos_emb = LearnedPositionalEmbedding(max_seq_len, embed_dim, dropout_prob=0.0)\n",
    "    else\n",
    "        pos_emb = SinusoidalPositionalEmbedding(max_seq_len, embed_dim, dropout_prob=0.0)\n",
    "    end\n",
    "    \n",
    "    emb_dropout = Dropout(dropout_prob)\n",
    "    \n",
    "    blocks = [\n",
    "        TransformerBlock(\n",
    "            embed_dim, \n",
    "            num_heads, \n",
    "            ffn_dim_mult=ffn_dim_mult, \n",
    "            dropout_prob=dropout_prob, \n",
    "            pre_norm=pre_norm,\n",
    "            use_rope=use_rope,\n",
    "            max_seq_len=max_seq_len,\n",
    "            use_flash_attn=use_flash_attn\n",
    "        ) for _ in 1:num_layers\n",
    "    ]\n",
    "    \n",
    "    ln_f = LayerNorm(embed_dim)\n",
    "    head = Dense(embed_dim, vocab_size)\n",
    "    \n",
    "    return AdvancedGPT(token_emb, pos_emb, blocks, ln_f, head, emb_dropout, config)\n",
    "end\n",
    "\n",
    "function (model::AdvancedGPT)(x::AbstractArray{Int}; return_all_layers=false)\n",
    "    # Support both single sequence (Vector{Int}) and batched input (Matrix{Int})\n",
    "    if ndims(x) == 1\n",
    "        # Single sequence: (T)\n",
    "        T = length(x)\n",
    "        batch_size = 1\n",
    "        x_reshaped = reshape(x, T, 1)\n",
    "    else\n",
    "        # Batch mode: (T, batch_size)\n",
    "        T, batch_size = size(x)\n",
    "        x_reshaped = x\n",
    "    end\n",
    "    \n",
    "    # Get token embeddings: shape (embed_dim, T, batch_size)\n",
    "    tok_emb = model.token_emb(x_reshaped)\n",
    "    \n",
    "    # Get positional embeddings: shape (embed_dim, T)\n",
    "    pos_emb = model.pos_emb(T)\n",
    "    \n",
    "    # Expand positional embeddings to match batch dimension\n",
    "    pos_emb_expanded = repeat(pos_emb, outer=[1, 1, batch_size])\n",
    "    \n",
    "    # Sum embeddings and apply dropout\n",
    "    h = model.emb_dropout(tok_emb .+ pos_emb_expanded)\n",
    "    \n",
    "    # Store intermediate activations if requested\n",
    "    activations = return_all_layers ? [h] : nothing\n",
    "    \n",
    "    # Pass through transformer blocks\n",
    "    for block in model.blocks\n",
    "        h = block(h)\n",
    "        if return_all_layers\n",
    "            push!(activations, h)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Final layer norm\n",
    "    h = model.ln_f(h)\n",
    "    \n",
    "    # Final projection to vocabulary logits for each position\n",
    "    logits = model.head(h)  # shape: (vocab_size, T, batch_size)\n",
    "    \n",
    "    # Permute to the standard NLP shape: (batch_size, vocab_size, T)\n",
    "    logits = permutedims(logits, (3, 1, 2))\n",
    "    \n",
    "    if return_all_layers\n",
    "        return logits, activations\n",
    "    else\n",
    "        return logits\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Text Generation Utilities\n",
    "# ---------------------------\n",
    "function softmax_safe(x)\n",
    "    if isa(x, Number)\n",
    "        return [1.0f0]  # Return single-element array for scalar input\n",
    "    end\n",
    "    # Ensure we're working with a vector\n",
    "    x_vec = vec(collect(Float32, x))\n",
    "    # Compute softmax\n",
    "    exp_x = exp.(x_vec .- maximum(x_vec))\n",
    "    return exp_x ./ sum(exp_x)\n",
    "end\n",
    "\n",
    "function generate(model::AdvancedGPT, \n",
    "                 prompt::Vector{Int}; \n",
    "                 max_new_tokens::Int=100, \n",
    "                 temperature::Float32=1.0f0,\n",
    "                 top_k::Int=0, \n",
    "                 top_p::Float32=0.9f0)\n",
    "    max_len = model.config[:max_seq_len]\n",
    "    tokens = copy(prompt)\n",
    "    \n",
    "    for _ in 1:max_new_tokens\n",
    "        # Truncate if exceeding context length\n",
    "        context = tokens[max(1, length(tokens) - max_len + 1):end]\n",
    "        context = reshape(context, :, 1)  # Make it (seq_len, batch_size)\n",
    "        \n",
    "        # Get logits for the next token\n",
    "        logits = model(context)  # (batch_size, vocab_size, seq_len)\n",
    "        \n",
    "        # Extract and ensure we have a vector\n",
    "        next_token_logits = Array{Float32}(vec(logits[1, :, end]))\n",
    "        \n",
    "        # Apply temperature\n",
    "        if temperature > 0\n",
    "            next_token_logits ./= temperature\n",
    "        else\n",
    "            # Greedy sampling\n",
    "            _, next_token = findmax(next_token_logits)\n",
    "            push!(tokens, next_token)\n",
    "            continue\n",
    "        end\n",
    "        \n",
    "        # Apply top-k filtering\n",
    "        if top_k > 0\n",
    "            # Keep only the top-k tokens\n",
    "            sorted_indices = sortperm(next_token_logits, rev=true)\n",
    "            next_token_logits[sorted_indices[top_k+1:end]] .= -Inf32\n",
    "        end\n",
    "        \n",
    "        # Apply top-p (nucleus) filtering\n",
    "        if top_p < 1.0\n",
    "            # Sort logits in descending order\n",
    "            sorted_indices = sortperm(next_token_logits, rev=true)\n",
    "            sorted_logits = next_token_logits[sorted_indices]\n",
    "            \n",
    "            # Calculate cumulative probabilities\n",
    "            exp_logits = exp.(sorted_logits .- maximum(sorted_logits))\n",
    "            probs = exp_logits ./ sum(exp_logits)\n",
    "            cumulative_probs = cumsum(probs)\n",
    "            \n",
    "            # Find cutoff index\n",
    "            cutoff_idx = findfirst(cumulative_probs .> top_p)\n",
    "            if !isnothing(cutoff_idx) && cutoff_idx > 1\n",
    "                next_token_logits[sorted_indices[cutoff_idx:end]] .= -Inf32\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        exp_logits = exp.(next_token_logits .- maximum(next_token_logits))\n",
    "        probs = exp_logits ./ sum(exp_logits)\n",
    "        \n",
    "        # Sample from the filtered distribution\n",
    "        next_token = sample(1:length(probs), Weights(probs))\n",
    "        push!(tokens, next_token)\n",
    "    end\n",
    "    \n",
    "    return tokens\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decode (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Tokenization (Simple Character-level for demonstration)\n",
    "# ---------------------------\n",
    "struct CharTokenizer\n",
    "    vocab::Dict{Char, Int}\n",
    "    idx_to_char::Dict{Int, Char}\n",
    "end\n",
    "\n",
    "function CharTokenizer(text::String)\n",
    "    unique_chars = unique(collect(text))\n",
    "    vocab = Dict(char => i for (i, char) in enumerate(unique_chars))\n",
    "    idx_to_char = Dict(i => char for (i, char) in enumerate(unique_chars))\n",
    "    return CharTokenizer(vocab, idx_to_char)\n",
    "end\n",
    "\n",
    "function encode(tokenizer::CharTokenizer, text::String)\n",
    "    return [tokenizer.vocab[c] for c in text if haskey(tokenizer.vocab, c)]\n",
    "end\n",
    "\n",
    "function decode(tokenizer::CharTokenizer, indices::Vector{Int})\n",
    "    return join([tokenizer.idx_to_char[i] for i in indices if haskey(tokenizer.idx_to_char, i)])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_gpt! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Training Loop\n",
    "# ---------------------------\n",
    "function train_gpt!(model::AdvancedGPT, data::Vector{Int}, \n",
    "                   opt::Flux.Optimise.AbstractOptimiser;\n",
    "                   batch_size::Int=16, \n",
    "                   bptt::Int=64,  # Sequence length for backprop through time\n",
    "                   epochs::Int=1,\n",
    "                   lr::Float64=3e-4,\n",
    "                   clip_norm::Float64=1.0)\n",
    "    \n",
    "    data_length = length(data)\n",
    "    \n",
    "    # Function to get a batch of sequences\n",
    "    function get_batch()\n",
    "        # Random starting indices\n",
    "        starts = rand(1:(data_length - bptt), batch_size)\n",
    "        \n",
    "        # Create input-target pairs\n",
    "        xs = zeros(Int, bptt, batch_size)\n",
    "        ys = zeros(Int, bptt, batch_size)\n",
    "        \n",
    "        for (i, start) in enumerate(starts)\n",
    "            end_idx = start + bptt - 1\n",
    "            xs[:, i] = data[start:end_idx]\n",
    "            ys[:, i] = data[(start+1):(end_idx+1)]\n",
    "        end\n",
    "        \n",
    "        return xs, ys\n",
    "    end\n",
    "    \n",
    "    # Training loop\n",
    "    steps_per_epoch = div(data_length, batch_size * bptt)\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    \n",
    "    params = Flux.params(model)\n",
    "    \n",
    "    for epoch in 1:epochs\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for step in 1:steps_per_epoch\n",
    "            # Get batch\n",
    "            x_batch, y_batch = get_batch()\n",
    "            \n",
    "            # Compute loss and gradients\n",
    "            loss, grads = Flux.withgradient(params) do\n",
    "                # Forward pass\n",
    "                logits = model(x_batch)  # (batch_size, vocab_size, seq_len)\n",
    "                \n",
    "                # Reshape for loss computation\n",
    "                logits_flat = reshape(permutedims(logits, (2, 3, 1)), :, batch_size)\n",
    "                targets_flat = reshape(y_batch, :)\n",
    "                \n",
    "                # Cross entropy loss\n",
    "                return crossentropy(logits_flat, targets_flat)\n",
    "            end\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if clip_norm > 0\n",
    "                Flux.Optimise.clip!(grads, clip_norm)\n",
    "            end\n",
    "            \n",
    "            # Update parameters\n",
    "            Flux.Optimise.update!(opt, params, grads)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            \n",
    "            if step % 10 == 0\n",
    "                println(\"Epoch: $epoch, Step: $step/$steps_per_epoch, Loss: $(loss)\")\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        avg_loss = epoch_loss / steps_per_epoch\n",
    "        println(\"Epoch $epoch completed. Average loss: $avg_loss\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: No functional GPU backend found! Defaulting to CPU.\n",
      "│ \n",
      "│ 1. If no GPU is available, nothing needs to be done.\n",
      "│ 2. If GPU is available, load the corresponding trigger package.\n",
      "│     a. `CUDA.jl` and `cuDNN.jl` (or just `LuxCUDA.jl`) for  NVIDIA CUDA Support.\n",
      "│     b. `AMDGPU.jl` for AMD GPU ROCM Support.\n",
      "│     c. `Metal.jl` for Apple Metal GPU Support. (Experimental)\n",
      "│     d. `oneAPI.jl` for Intel oneAPI GPU Support. (Experimental)\n",
      "└ @ MLDataDevices.Internal C:\\Users\\khoj\\.julia\\packages\\MLDataDevices\\Cq9gx\\src\\internal.jl:94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (2, 50000, 10)\n",
      "Generated token IDs: [10, 20, 30, 40, 50, 28710, 38915, 23406, 43173, 43504, 34141, 2594, 27379, 21366, 26905, 5922, 13592, 30117, 12022, 143, 1194, 40307, 27259, 5920, 1993]\n",
      "Model successfully created and tested!\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Advanced Usage Example\n",
    "# ---------------------------\n",
    "function main()\n",
    "    # Parameters for a medium-size model\n",
    "    config = Dict(\n",
    "        :vocab_size => 50000,\n",
    "        :max_seq_len => 1024,\n",
    "        :embed_dim => 768,\n",
    "        :num_heads => 12,\n",
    "        :num_layers => 12,\n",
    "        :dropout_prob => 0.1,\n",
    "        :pos_emb_type => :learned,\n",
    "        :pre_norm => true,\n",
    "        :use_rope => true,\n",
    "        :use_flash_attn => false,  # Set to true if GPU available\n",
    "        :ffn_dim_mult => 4\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = AdvancedGPT(;\n",
    "        vocab_size=config[:vocab_size],\n",
    "        max_seq_len=config[:max_seq_len],\n",
    "        embed_dim=config[:embed_dim],\n",
    "        num_heads=config[:num_heads],\n",
    "        num_layers=config[:num_layers],\n",
    "        dropout_prob=config[:dropout_prob],\n",
    "        pos_emb_type=config[:pos_emb_type],\n",
    "        pre_norm=config[:pre_norm],\n",
    "        use_rope=config[:use_rope],\n",
    "        use_flash_attn=config[:use_flash_attn],\n",
    "        ffn_dim_mult=config[:ffn_dim_mult]\n",
    "    )\n",
    "    \n",
    "    # Example - move to GPU if available\n",
    "    if CUDA.functional()\n",
    "        println(\"Using CUDA...\")\n",
    "        model = model |> gpu\n",
    "    end\n",
    "    \n",
    "    # Sample inputs (for demonstration)\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    input_ids = rand(1:config[:vocab_size], seq_len, batch_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(input_ids)\n",
    "    println(\"Logits shape: \", size(logits))  # Should be (batch_size, vocab_size, seq_len)\n",
    "    \n",
    "    # Generate text example\n",
    "    # Note: In a real scenario, you would use a proper tokenizer\n",
    "    prompt = [10, 20, 30, 40, 50]  # Example token IDs\n",
    "    generated = generate(model, prompt, max_new_tokens=20, temperature=0.7f0, top_p=0.9f0)\n",
    "    println(\"Generated token IDs: \", generated)\n",
    "    \n",
    "    println(\"Model successfully created and tested!\")\n",
    "end\n",
    "\n",
    "# Call main to test\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Logits shape: (2, 50000, 10)\"\n",
    "# Shows correct dimensions:\n",
    "   # 2: batch size\n",
    "   # 50000: vocabulary size\n",
    "   # 10: sequence length\n",
    "\n",
    "# \"Generated token IDs:\" shows:\n",
    "   # First 5 numbers [10, 20, 30, 40, 50] are your original prompt\n",
    "   # Following numbers are the generated tokens\n",
    "   # The generation looks reasonable with diverse token IDs within the vocabulary range (0-50000)\n",
    "\n",
    "# \"Model successfully created and tested!\" - Confirms the model initialization and forward pass worked correctly\n",
    "\n",
    "# If you want to see the actual text output, you would need to pass these tokens through your tokenizer's decode function to convert them back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GPT model example...\n",
      "Vocabulary size: 28\n",
      "Using CUDA...\n",
      "\n",
      "Prompt: \"Julia is\"\n",
      "\n",
      "Generated text:\n",
      "Julia isrrrrrhhpmf,rhrhfamW,JhwnWJmfpfawaWfhopffaa.rprrlgf\n",
      "\n",
      "Model statistics:\n",
      "- Vocabulary size: 28\n",
      "- Context length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `Flux.params(m...)` is deprecated. Use `Flux.trainable(model)` for parameter collection,\n",
      "│ and the explicit `gradient(m -> loss(m, x, y), model)` for gradient computation.\n",
      "└ @ Flux C:\\Users\\khoj\\.julia\\packages\\Flux\\Sgc17\\src\\deprecations.jl:93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Number of parameters: 6389788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(AdvancedGPT(TokenEmbedding(Float32[0.090890154 -0.099342726 … 0.02606686 -0.10358236; 0.07702502 0.07856488 … -0.107408084 0.04179199; … ; 0.09270245 0.066110544 … 0.014684047 0.12216856; 0.06867142 -0.100097716 … 0.14114578 0.084149435], Dropout(0.0)), LearnedPositionalEmbedding(Float32[0.084296644 0.11054972 … 0.017054155 -0.0199074; -0.12008329 0.10933383 … -0.06468785 -0.030502647; … ; -0.010624945 0.071070075 … -0.049582854 0.0067366958; 0.12292118 0.114829466 … 0.07636237 -0.0025499314], Dropout(0.0)), TransformerBlock[TransformerBlock(MultiHeadAttention(Dense(256 => 256), Dense(256 => 256), Dense(256 => 256), Dense(256 => 256), 8, 32, Dropout(0.1), Dropout(0.1), RotaryPositionEmbedding(32, 128, Float32[0.5403023 -0.41614684 … 0.2323591 -0.6928958; 0.84600914 0.43146282 … -0.66799676 -0.9618962; … ; 0.99999994 0.9999998 … 0.99919367 0.9991809; 1.0 0.99999994 … 0.999745 0.99974096], Float32[0.84147096 0.9092974 … 0.9726301 0.7210377; 0.53316844 0.9021307 … 0.74416417 0.27341488; … ; 0.00031622776 0.0006324555 … 0.040150132 0.040466104; 0.00017782794 0.00035565588 … 0.02258223 0.022760011]), false), LayerNorm(256), SwiGLU(Dense(256 => 1024), Dense(256 => 1024), Dense(1024 => 256), Dropout(0.1)), LayerNorm(256), true), TransformerBlock(MultiHeadAttention(Dense(256 => 256), Dense(256 => 256), Dense(256 => 256), Dense(256 => 256), 8, 32, Dropout(0.1), Dropout(0.1), RotaryPositionEmbedding(32, 128, Float32[0.5403023 -0.41614684 … 0.2323591 -0.6928958; 0.84600914 0.43146282 … -0.66799676 -0.9618962; … ; 0.99999994 0.9999998 … 0.99919367 0.9991809; 1.0 0.99999994 … 0.999745 0.99974096], Float32[0.84147096 0.9092974 … 0.9726301 0.7210377; 0.53316844 0.9021307 … 0.74416417 0.27341488; … ; 0.00031622776 0.0006324555 … 0.040150132 0.040466104; 0.00017782794 0.00035565588 … 0.02258223 0.022760011]), false), LayerNorm(256), SwiGLU(Dense(256 => 1024), Dense(256 => 1024), Dense(1024 => 256), Dropout(0.1)), LayerNorm(256), true), TransformerBlock(MultiHeadAttention(Dense(256 => 256), Dense(256 => 256), Dense(256 => 256), Dense(256 => 256), 8, 32, Dropout(0.1), Dropout(0.1), RotaryPositionEmbedding(32, 128, Float32[0.5403023 -0.41614684 … 0.2323591 -0.6928958; 0.84600914 0.43146282 … -0.66799676 -0.9618962; … ; 0.99999994 0.9999998 … 0.99919367 0.9991809; 1.0 0.99999994 … 0.999745 0.99974096], Float32[0.84147096 0.9092974 … 0.9726301 0.7210377; 0.53316844 0.9021307 … 0.74416417 0.27341488; … ; 0.00031622776 0.0006324555 … 0.040150132 0.040466104; 0.00017782794 0.00035565588 … 0.02258223 0.022760011]), false), LayerNorm(256), SwiGLU(Dense(256 => 1024), Dense(256 => 1024), Dense(1024 => 256), Dropout(0.1)), LayerNorm(256), true), TransformerBlock(MultiHeadAttention(Dense(256 => 256), Dense(256 => 256), Dense(256 => 256), Dense(256 => 256), 8, 32, Dropout(0.1), Dropout(0.1), RotaryPositionEmbedding(32, 128, Float32[0.5403023 -0.41614684 … 0.2323591 -0.6928958; 0.84600914 0.43146282 … -0.66799676 -0.9618962; … ; 0.99999994 0.9999998 … 0.99919367 0.9991809; 1.0 0.99999994 … 0.999745 0.99974096], Float32[0.84147096 0.9092974 … 0.9726301 0.7210377; 0.53316844 0.9021307 … 0.74416417 0.27341488; … ; 0.00031622776 0.0006324555 … 0.040150132 0.040466104; 0.00017782794 0.00035565588 … 0.02258223 0.022760011]), false), LayerNorm(256), SwiGLU(Dense(256 => 1024), Dense(256 => 1024), Dense(1024 => 256), Dropout(0.1)), LayerNorm(256), true), TransformerBlock(MultiHeadAttention(Dense(256 => 256), Dense(256 => 256), Dense(256 => 256), Dense(256 => 256), 8, 32, Dropout(0.1), Dropout(0.1), RotaryPositionEmbedding(32, 128, Float32[0.5403023 -0.41614684 … 0.2323591 -0.6928958; 0.84600914 0.43146282 … -0.66799676 -0.9618962; … ; 0.99999994 0.9999998 … 0.99919367 0.9991809; 1.0 0.99999994 … 0.999745 0.99974096], Float32[0.84147096 0.9092974 … 0.9726301 0.7210377; 0.53316844 0.9021307 … 0.74416417 0.27341488; … ; 0.00031622776 0.0006324555 … 0.040150132 0.040466104; 0.00017782794 0.00035565588 … 0.02258223 0.022760011]), false), LayerNorm(256), SwiGLU(Dense(256 => 1024), Dense(256 => 1024), Dense(1024 => 256), Dropout(0.1)), LayerNorm(256), true), TransformerBlock(MultiHeadAttention(Dense(256 => 256), Dense(256 => 256), Dense(256 => 256), Dense(256 => 256), 8, 32, Dropout(0.1), Dropout(0.1), RotaryPositionEmbedding(32, 128, Float32[0.5403023 -0.41614684 … 0.2323591 -0.6928958; 0.84600914 0.43146282 … -0.66799676 -0.9618962; … ; 0.99999994 0.9999998 … 0.99919367 0.9991809; 1.0 0.99999994 … 0.999745 0.99974096], Float32[0.84147096 0.9092974 … 0.9726301 0.7210377; 0.53316844 0.9021307 … 0.74416417 0.27341488; … ; 0.00031622776 0.0006324555 … 0.040150132 0.040466104; 0.00017782794 0.00035565588 … 0.02258223 0.022760011]), false), LayerNorm(256), SwiGLU(Dense(256 => 1024), Dense(256 => 1024), Dense(1024 => 256), Dropout(0.1)), LayerNorm(256), true)], LayerNorm(256), Dense(256 => 28), Dropout(0.1), Dict{Symbol, Any}(:vocab_size => 28, :num_heads => 8, :use_flash_attn => false, :embed_dim => 256, :dropout_prob => 0.1, :ffn_dim_mult => 4, :pre_norm => true, :max_seq_len => 128, :pos_emb_type => :learned, :num_layers => 6…)), CharTokenizer(Dict('n' => 19, 'f' => 16, 'w' => 28, 'd' => 21, 'e' => 11, 'o' => 17, '\\n' => 24, 'h' => 8, 's' => 7, 'i' => 4…), Dict(5 => 'a', 16 => 'f', 20 => 'c', 12 => 'v', 24 => '\\n', 28 => 'w', 8 => 'h', 17 => 'o', 1 => 'J', 19 => 'n'…)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I'll create an example that demonstrates how to use the model with actual text data. Here's a comprehensive example:\n",
    "\n",
    "function example_usage()\n",
    "    println(\"Starting GPT model example...\")\n",
    "    \n",
    "    # 1. Define sample text for training/testing\n",
    "    sample_text = \"\"\"\n",
    "    Julia is a high-level, high-performance, dynamic programming language. \n",
    "    While it is a general-purpose language and can be used to write any application, \n",
    "    many of its features are well suited for numerical analysis and computational science.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 2. Create tokenizer and encode sample text\n",
    "    tokenizer = CharTokenizer(sample_text)\n",
    "    println(\"Vocabulary size: \", length(tokenizer.vocab))\n",
    "    \n",
    "    # 3. Create model with appropriate size for this example\n",
    "    model = AdvancedGPT(\n",
    "        vocab_size=length(tokenizer.vocab),\n",
    "        max_seq_len=128,\n",
    "        embed_dim=256,      # Smaller for this example\n",
    "        num_heads=8,\n",
    "        num_layers=6,       # Reduced layers for faster execution\n",
    "        dropout_prob=0.1,\n",
    "        pos_emb_type=:learned,\n",
    "        pre_norm=true,\n",
    "        use_rope=true,\n",
    "        use_flash_attn=false\n",
    "    )\n",
    "    \n",
    "    # 4. Move to GPU if available\n",
    "    if CUDA.functional()\n",
    "        println(\"Using CUDA...\")\n",
    "        model = model |> gpu\n",
    "    end\n",
    "    \n",
    "    # 5. Generate text from a prompt\n",
    "    prompt = \"Julia is\"\n",
    "    prompt_tokens = encode(tokenizer, prompt)\n",
    "    println(\"\\nPrompt: \\\"$prompt\\\"\")\n",
    "    \n",
    "    # 6. Generate continuation\n",
    "    generated_tokens = generate(\n",
    "        model,\n",
    "        prompt_tokens;\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7f0,\n",
    "        top_k=50,\n",
    "        top_p=0.9f0\n",
    "    )\n",
    "    \n",
    "    # 7. Decode and display the result\n",
    "    generated_text = decode(tokenizer, generated_tokens)\n",
    "    println(\"\\nGenerated text:\")\n",
    "    println(generated_text)\n",
    "    \n",
    "    # 8. Show some model statistics\n",
    "    println(\"\\nModel statistics:\")\n",
    "    println(\"- Vocabulary size: \", length(tokenizer.vocab))\n",
    "    println(\"- Context length: \", model.config[:max_seq_len])\n",
    "    println(\"- Number of parameters: \", sum(length, Flux.params(model)))\n",
    "    \n",
    "    return model, tokenizer\n",
    "end\n",
    "\n",
    "# Run the example\n",
    "model, tokenizer = example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: \"Julia can\"\n",
      "Generated: Julia canrrr-hysrrbhri,r u,wrh,ppcwvgiWagabsoipyr\n",
      "iiprWrWWy\n",
      "\n",
      "Prompt: \"Programming in\"\n",
      "Generated: rogramming in,Wbnose,hphsWoy lmsil.cl,bnfvrWh Jfnsh,iWmvrvhv pW\n",
      "\n",
      "Prompt: \"The language\"\n",
      "Generated: he languageblrl-or,btny,nc,rrwv,lwyncsnhw,wbw,hprrncnrbns pll\n"
     ]
    }
   ],
   "source": [
    "# Generate more text with different prompts\n",
    "function generate_text(model, tokenizer, prompt; max_tokens=50)\n",
    "    tokens = encode(tokenizer, prompt)\n",
    "    generated = generate(\n",
    "        model,\n",
    "        tokens,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7f0,\n",
    "        top_k=50,\n",
    "        top_p=0.9f0\n",
    "    )\n",
    "    return decode(tokenizer, generated)\n",
    "end\n",
    "\n",
    "# Try different prompts - Different prompt handling\n",
    "prompts = [\n",
    "    \"Julia can\",\n",
    "    \"Programming in\",\n",
    "    \"The language\"\n",
    "]\n",
    "\n",
    "for prompt in prompts\n",
    "    println(\"\\nPrompt: \\\"$prompt\\\"\")\n",
    "    println(\"Generated: \", generate_text(model, tokenizer, prompt))\n",
    "end\n",
    "\n",
    "# Note that since we're using a character-level tokenizer and a relatively small model, the generated text might not be very coherent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
