{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee91e2-c60a-4ad1-9de9-b9db13e5067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example of how you might implement a very basic GPT‐style transformer language model in Julia using Flux. \n",
    "# This code builds on the ideas of embedding tokens and positions, stacking a few transformer blocks (each with multi‐head self-attention and a feedforward MLP), and finally projecting the output to a vocabulary space. \n",
    "# (Note that this is a simplified example intended for educational purposes; real-world GPT models include many more details and optimizations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c96266c-f4c3-4e84-a3e7-a8b710ddafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle, glorot_uniform, @functor\n",
    "using Statistics\n",
    "using StatsBase: sample, Weights\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "170cef4a-5d55-42f4-acda-c83d8a35005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mThe use of `Flux.@functor` is deprecated.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mMost likely, you should write `Flux.@layer MyLayer`which will add various convenience methods for your type,such as pretty-printing and use with Adapt.jl.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mHowever, this is not required. Flux.jl v0.15 uses Functors.jl v0.5,which makes exploration of most nested `struct`s opt-out instead of opt-in...so Flux will automatically see inside any custom struct definitions.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mIf you really want to apply the `@functor` macro to a custom struct, use `Functors.@functor` instead.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Flux C:\\Users\\khoj\\.julia\\packages\\Flux\\Sgc17\\src\\deprecations.jl:101\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Token and Positional Embeddings\n",
    "# ---------------------------\n",
    "\n",
    "    # Embeddings:\n",
    "    # Two types of embeddings are defined:\n",
    "        # TokenEmbedding: Maps token indices to dense vectors.\n",
    "        # PositionalEmbedding: Provides a learnable positional encoding that is added to the token embeddings.\n",
    "\n",
    "struct TokenEmbedding\n",
    "    emb::Array{Float32,2}\n",
    "end\n",
    "@functor TokenEmbedding\n",
    "\n",
    "TokenEmbedding(vocab_size::Int, embed_dim::Int) = TokenEmbedding(glorot_uniform(embed_dim, vocab_size))\n",
    "\n",
    "function (te::TokenEmbedding)(x::Vector{Int})\n",
    "    # Returns embedding of shape (embed_dim, sequence_length)\n",
    "    return te.emb[:, x]\n",
    "end\n",
    "\n",
    "struct PositionalEmbedding\n",
    "    emb::Array{Float32,2}\n",
    "end\n",
    "@functor PositionalEmbedding\n",
    "\n",
    "PositionalEmbedding(seq_len::Int, embed_dim::Int) = PositionalEmbedding(glorot_uniform(embed_dim, seq_len))\n",
    "\n",
    "function (pe::PositionalEmbedding)(T::Int)\n",
    "    # Return positional embeddings for positions 1:T\n",
    "    return pe.emb[:, 1:T]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c587fba-025f-49d4-a995-9933d65e1457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Multi-Head Self-Attention Layer\n",
    "# ---------------------------\n",
    "\n",
    "    # Multi-Head Self-Attention:\n",
    "    # The MultiHeadSelfAttention struct implements attention by first projecting inputs into queries, keys, and values, splitting them into heads, computing scaled dot-product attention (with a causal mask so that each token only attends to previous tokens), and then concatenating and projecting the result.\n",
    "\n",
    "struct MultiHeadSelfAttention\n",
    "    W_q::Dense\n",
    "    W_k::Dense\n",
    "    W_v::Dense\n",
    "    W_o::Dense\n",
    "    num_heads::Int\n",
    "end\n",
    "\n",
    "function MultiHeadSelfAttention(embed_dim::Int, num_heads::Int)\n",
    "    return MultiHeadSelfAttention(\n",
    "        Dense(embed_dim, embed_dim),\n",
    "        Dense(embed_dim, embed_dim),\n",
    "        Dense(embed_dim, embed_dim),\n",
    "        Dense(embed_dim, embed_dim),\n",
    "        num_heads\n",
    "    )\n",
    "end\n",
    "\n",
    "function (mha::MultiHeadSelfAttention)(x::Array{Float32,3})\n",
    "    # x has shape (embed_dim, T, batch_size)\n",
    "    embed_dim, T, batch_size = size(x)\n",
    "    head_dim = div(embed_dim, mha.num_heads)\n",
    "    \n",
    "    # Compute Q, K, V; shape: (embed_dim, T, batch_size)\n",
    "    Q = mha.W_q(x)\n",
    "    K = mha.W_k(x)\n",
    "    V = mha.W_v(x)\n",
    "    \n",
    "    # Reshape to (head_dim, num_heads, T, batch_size)\n",
    "    function split_heads(t)\n",
    "        return reshape(t, head_dim, mha.num_heads, T, batch_size)\n",
    "    end\n",
    "    Qh = split_heads(Q)\n",
    "    Kh = split_heads(K)\n",
    "    Vh = split_heads(V)\n",
    "    \n",
    "    # Compute scaled dot-product attention for each head\n",
    "    attn = Array{Float32}(undef, T, T, batch_size, mha.num_heads)\n",
    "    for b in 1:batch_size, h in 1:mha.num_heads\n",
    "        # Q and K for this head have shape (head_dim, T)\n",
    "        Q_temp = Qh[:, h, :, b]  # (head_dim, T)\n",
    "        K_temp = Kh[:, h, :, b]  # (head_dim, T)\n",
    "        # Compute similarity scores: (T, T)\n",
    "        scores = (Q_temp' * K_temp) ./ sqrt(head_dim)\n",
    "        # Apply causal mask: prevent attending to future tokens\n",
    "        for i in 1:T, j in i+1:T\n",
    "            scores[i,j] = -Inf32\n",
    "        end\n",
    "        attn[:,:,b,h] = softmax(scores, dims=1)\n",
    "    end\n",
    "    \n",
    "    # Multiply by V: for each head compute weighted sum of V along time dimension\n",
    "    out = zeros(Float32, head_dim, T, batch_size, mha.num_heads)\n",
    "    for b in 1:batch_size, h in 1:mha.num_heads\n",
    "        V_temp = Vh[:, h, :, b]  # (head_dim, T)\n",
    "        out[:, :, b, h] = V_temp * attn[:,:,b,h]\n",
    "    end\n",
    "    \n",
    "    # Concatenate heads: reshape from (head_dim, T, batch_size, num_heads) to (embed_dim, T, batch_size)\n",
    "    out = reshape(out, embed_dim, T, batch_size)\n",
    "    # Final linear projection\n",
    "    return mha.W_o(out)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c5a69c1-d35a-41c9-a71c-7a7bc5f69141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Transformer Block (GPT Block)\n",
    "# ---------------------------\n",
    "\n",
    "    # Transformer Block:\n",
    "    # Each block applies layer normalization, self-attention, a residual connection, a feedforward MLP (with its own layer normalization and residual connection), mimicking the GPT block design.\n",
    "\n",
    "struct TransformerBlock\n",
    "    attn::MultiHeadSelfAttention\n",
    "    ln1::LayerNorm\n",
    "    mlp::Chain\n",
    "    ln2::LayerNorm\n",
    "end\n",
    "\n",
    "function TransformerBlock(embed_dim::Int, num_heads::Int; dropout_prob=0.1)\n",
    "    attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "    ln1 = LayerNorm(embed_dim)\n",
    "    mlp = Chain(\n",
    "        Dense(embed_dim, 4 * embed_dim, relu),\n",
    "        Dense(4 * embed_dim, embed_dim)\n",
    "    )\n",
    "    ln2 = LayerNorm(embed_dim)\n",
    "    return TransformerBlock(attn, ln1, mlp, ln2)\n",
    "end\n",
    "\n",
    "function (block::TransformerBlock)(x::Array{Float32,3})\n",
    "    # x: (embed_dim, T, batch_size)\n",
    "    x_attn = block.attn(block.ln1(x))\n",
    "    x = x .+ x_attn\n",
    "    x_mlp = block.mlp(block.ln2(x))\n",
    "    return x .+ x_mlp\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "162aecc4-a3d3-43af-9a70-a0fa1c660f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# GPT Model\n",
    "# ---------------------------\n",
    "\n",
    "    # GPT Model Structure:\n",
    "    # The model stacks token and positional embeddings followed by several transformer blocks. A final layer normalization and linear projection produce logits over the vocabulary for each position.\n",
    "\n",
    "struct GPT\n",
    "    token_emb::TokenEmbedding\n",
    "    pos_emb::PositionalEmbedding\n",
    "    blocks::Vector{TransformerBlock}\n",
    "    ln_f::LayerNorm\n",
    "    head::Dense\n",
    "end\n",
    "\n",
    "function GPT(vocab_size::Int, seq_len::Int, embed_dim::Int, num_heads::Int, num_layers::Int)\n",
    "    token_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "    pos_emb = PositionalEmbedding(seq_len, embed_dim)\n",
    "    blocks = [TransformerBlock(embed_dim, num_heads) for _ in 1:num_layers]\n",
    "    ln_f = LayerNorm(embed_dim)\n",
    "    head = Dense(embed_dim, vocab_size)\n",
    "    return GPT(token_emb, pos_emb, blocks, ln_f, head)\n",
    "end\n",
    "\n",
    "function (model::GPT)(x::Vector{Int})\n",
    "    # x is a vector of token indices (length T). For simplicity, we assume batch size 1.\n",
    "    T = length(x)\n",
    "    # Get token embeddings: shape (embed_dim, T)\n",
    "    tok_emb = model.token_emb(x)\n",
    "    # Get positional embeddings: shape (embed_dim, T)\n",
    "    pos_emb = model.pos_emb(T)\n",
    "    # Sum embeddings\n",
    "    h = tok_emb .+ pos_emb\n",
    "    # Add batch dimension: shape (embed_dim, T, 1)\n",
    "    h = reshape(h, size(h,1), size(h,2), 1)\n",
    "    # Pass through transformer blocks\n",
    "    for block in model.blocks\n",
    "        h = block(h)\n",
    "    end\n",
    "    h = model.ln_f(h)\n",
    "    # Final projection to vocabulary logits for each position\n",
    "    logits = model.head(h)  # shape: (vocab_size, T, 1)\n",
    "    return logits\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b44330ac-3505-4b1d-8233-ba6de05f8229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (100, 20, 1)\n",
      "Predicted next token index: 13\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Example Usage\n",
    "# ---------------------------\n",
    "\n",
    "     # A toy model is defined with small hyperparameters and vocabulary size. A dummy input sequence is fed through the model, and the code prints the shape of the logits. It also demonstrates how to sample a predicted next token from the logits.\n",
    "\n",
    "# Define hyperparameters for a toy model\n",
    "vocab_size = 100       # For demonstration; in practice, use a larger vocabulary.\n",
    "seq_len = 20           # Maximum sequence length (block size)\n",
    "embed_dim = 32         # Embedding (hidden) dimension\n",
    "num_heads = 4          # Number of attention heads\n",
    "num_layers = 2         # Number of transformer blocks\n",
    "\n",
    "# Create a GPT model instance\n",
    "gpt_model = GPT(vocab_size, seq_len, embed_dim, num_heads, num_layers)\n",
    "\n",
    "# Create a dummy input: a sequence of token indices (length = seq_len)\n",
    "input_sequence = rand(1:vocab_size, seq_len)\n",
    "\n",
    "# Pass the sequence through the model to obtain logits\n",
    "logits = gpt_model(input_sequence)\n",
    "println(\"Logits shape: \", size(logits))  # Expect (vocab_size, seq_len, 1)\n",
    "\n",
    "# For instance, to predict the next token for the last position, one could take:\n",
    "predicted_distribution = softmax(logits[:, end, 1])\n",
    "predicted_token = sample(1:vocab_size, Weights(vec(predicted_distribution)))\n",
    "println(\"Predicted next token index: \", predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d38d4-5460-4b63-b320-4616f88d0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output is exactly what we would expect from this toy model demonstration:\n",
    "\n",
    "    # Logits Shape (100, 20, 1):\n",
    "    # This indicates that for a batch size of 1, the model produces outputs for 20 tokens (the sequence length), with each output being a vector of length 100. The 100 corresponds to the vocabulary size, meaning the model outputs a score (logit) for each token in the vocabulary at each position.\n",
    "\n",
    "    # Predicted Next Token Index (13):\n",
    "    # After applying the softmax to the logits of the final token, the model samples a token index. In this case, token 13 was chosen. Since the model is randomly initialized (and likely untrained), this number is arbitrary but confirms that the prediction mechanism is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1fd5aa-2e1f-41e4-a328-408800fdab62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
